<!DOCTYPE html>
<html>
<head>
	<title>Algebra for Programmers</title>
	<link rel="stylesheet" href="default.min.css">
	<style type="text/css">
		h1 { text-align: center; }
		h2 { text-align: center; }
		h3 { text-align: center; }
		dd {
			margin-bottom: 1em;
			margin-top: 0.5em;
		}
	</style>
	<script src="highlight.min.js"></script>
	<script src="jquery-1.8.2.min.js"></script>
	<script src="slideshow.js"></script>
	<script>hljs.initHighlightingOnLoad();</script>
	<script>
		$(document).ready(function () {
			slideshow();
		});
</script>
</head>
<body>
<div class="slide">
	<h1>Algebra for Programmers</h1>
	<h2>Emily Estes</h2>
	<h3>2016-??-?? @ Functional Knox Meetup</h3>
	<p>To navigate: left arrow - previous slide, right arrow - next slide, escape - toggle slide show mode.</p>
</div>


<div class="slide">
<h1>Goals of this presentation</h1>
<ul>
	<li>Explain terminology frequently used in discussion of functional programming, in particular terms from algebra and (some) terms of category theory.</li>
	<li>Provide a review of relevant topics in elementary algebra(i.e. what is typically covered in high school algebra classes).</li>
	<li>Connect algebraic topics to their appearance in functional programming.</li>
	<li>Demonstrate that building programs out of smaller programs using reliable combining operations is "doing algebra". (Or rather, algebra studies primitives that can help us study how to do this better.)</li>
	<li>Try to help alleviate "jargon overload" to people new to functional programming...</li>
</ul>
<blockquote>1990 - A committee formed by Simon Peyton-Jones, Paul Hudak, Philip Wadler, Ashton Kutcher, and People for the Ethical Treatment of Animals creates Haskell, a pure, non-strict, functional language. Haskell gets some resistance due to the complexity of using monads to control side effects. Wadler tries to appease critics by explaining that <em>"a monad is a monoid in the category of endofunctors, what's the problem?"</em> - from <a href="http://james-iry.blogspot.com/2009/05/brief-incomplete-and-mostly-wrong.html">A Brief, Incomplete, and Mostly Wrong History of Programming Languages</a></blockquote>
</div>



<div class="slide">
<h1>Question: How do you write a big program?</h1>
</div>



<div class="slide">
<h1>Question: How do you write a big program?</h1>
<h1>Answer: By combining multiple little programs.</h1>
</div>



<div class="slide">
<h1>Question: How do you write a big program?</h1>
<h1>Answer: By combining multiple little programs.</h1>
<h1>Question: How do you combine little programs?</h1>
</div>


<div class="slide">
<h1>Question: How do you write a big program?</h1>
<h1>Answer: By combining multiple little programs.</h1>
<h1>Question: How do you combine little programs?</h1>
<h1>Answer: With another program.</h1>
</div>



<div class="slide">
<h1>Question: How do you write a big program?</h1>
<h1>Answer: By combining multiple little programs.</h1>
<h1>Question: How do you combine little programs?</h1>
<h1>Answer: With another program.</h1>
<h1>Question: Wait a minute... what's a program?</h1>
</div>



<div class="slide">
<h1>Question: How do you write a big program?</h1>
<h1>Answer: By combining multiple little programs.</h1>
<h1>Question: How do you combine little programs?</h1>
<h1>Answer: With another program.</h1>
<h1>Question: Wait a minute... what's a program?</h1>
<h1>Answer:</h1>
<p>Back in the 1930's some Mathematicians decided to call "programs"(or algorithms) the set of Computable functions, which are a subset of "all possible functions". Being mathematicians, the next step was to figure out which functions from the set of all possible functions are computable. We know some functions are computable and some are not. In particular, there are two "models" of computable functions: Turing Machines (discovered by Alan Turing), and expressions in the Lambda Calculus (discovered by Alonzo Church). It turns out that these two models are equivalent (every function that can be computed by a Turing Machine has an expression in the Lambda calculus, and every expression in the Lambda calculus can be computed by a Turing machine). Every function that we have found to be computable can be expressed by those models, however, we haven't proved that these models are capable of expressing <em>every</em> computable function. However, it is generally accepted that they do, and this is called the <strong>Church-Turing Thesis</strong>.</p>
</div>



<div class="slide">
<h1>Question: How do you write a big program?</h1>
<h1>Answer: By combining multiple little programs.</h1>
<h1>Question: How do you combine little programs?</h1>
<h1>Answer: With another program.</h1>
<h1>Question: Wait a minute... what's a program?</h1>
<h1>Answer:</h1>
<p>Back in the 1930's some Mathematicians decided to call "programs"(or algorithms) the set of Computable functions, which are a subset of "all possible functions". Being mathematicians, the next step was to figure out which functions from the set of all possible functions are computable. We know some functions are computable and some are not. In particular, there are two "models" of computable functions: Turing Machines (discovered by Alan Turing), and expressions in the Lambda Calculus (discovered by Alonzo Church). It turns out that these two models are equivalent (every function that can be computed by a Turing Machine has an expression in the Lambda calculus, and every expression in the Lambda calculus can be computed by a Turing machine). Every function that we have found to be computable can be expressed by those models, however, we haven't proved that these models are capable of expressing <em>every</em> computable function. However, it is generally accepted that they do, and this is called the <strong>Church-Turing Thesis</strong>.</p>
<h1>Question: Do you have any tylenol?</h1>
</div>



<div class="slide">
<h1>Question: How do you write a big program?</h1>
<h1>Answer: By combining multiple little programs.</h1>
<h1>Question: How do you combine little programs?</h1>
<h1>Answer: With another program.</h1>
<h1>Question: Wait a minute... what's a program?</h1>
<h1>Answer:</h1>
<p>Back in the 1930's some Mathematicians decided to call "programs"(or algorithms) the set of Computable functions, which are a subset of "all possible functions". Being mathematicians, the next step was to figure out which functions from the set of all possible functions are computable. We know some functions are computable and some are not. In particular, there are two "models" of computable functions: Turing Machines (discovered by Alan Turing), and expressions in the Lambda Calculus (discovered by Alonzo Church). It turns out that these two models are equivalent (every function that can be computed by a Turing Machine has an expression in the Lambda calculus, and every expression in the Lambda calculus can be computed by a Turing machine). Every function that we have found to be computable can be expressed by those models, however, we haven't proved that these models are capable of expressing <em>every</em> computable function. However, it is generally accepted that they do, and this is called the <strong>Church-Turing Thesis</strong>.</p>
<h1>Question: Do you have any tylenol?</h1>
<h1>Answer: Ah... nevermind. Programs are functions.</h1>
</div>



<div class="slide">
<h1>So let's go back to combining programs... I mean functions. You said we're going to use functions?</h1>

</div>



<div class="slide">
<h1>A "multithreaded" program.</h1>
<p>Let's start with a simple class to keep a counter.</p>
<pre><code class="java">final class Counter {
	private int counter;
	public Counter(int initial) { this.counter = initial; }
	public int increment() { return this.counter++; }
	public int decrement() { return this.counter--; }
	public int get() { return this.counter; }
	public int set(int value) { return this.counter = value; }
}</code></pre>
<p>Now, we want to make two counters, and "drain" one to another. And we would like to do this concurrently, because maybe we're building a work queue of some sort, and want to keep a list of completed jobs. Now, since there are going to be two counters(or lists) that other threads may be accessing, we want to use a lock to keep other threads from catching us altering the states of both counters. So let's write a simple class to do this. Also, since these things can take some time, we'll use some calls to <code>Thread.sleep(...)</code> to symbolize this.</p>
<pre><code class="java">final class Drain implements Runnable {
	private final Counter source;
	private final Counter sink;
	public Drain(Counter source, Counter sink) {
		this.source = source;
		this.sink = sink;
	}
	@Override public void run() {
		boolean flag = true;
		while(flag) {
			synchronized(source) {
				if(source.get() > 0) {
					synchronized(sink) {
						try { Thread.sleep(150); } catch (InterruptedException ie) { }
						source.decrement();
						sink.increment();
						System.out.println("Drain(" + this + "): " + source.get() + ", " + sink.get());
					}
				} else {
					flag = false;
				}
				try { Thread.sleep(40); } catch (InterruptedException ie) { }
			}
		}
	}
}</code></pre>
</div>

<div>
<h1>Running our multithreaded program "solo"</h1>
<p>So let's make a program to use our classes.</p>
<pre><code class="java">public final class Threads {
	public static void main(String[] args) throws Exception {
		final Counter a = new Counter(5);
		final Counter b = new Counter(10);
		final Drain d1 = new Drain(a, b);
		System.out.println("Counters: " + a.get() + ", " + b.get());
		final Thread t1 = new Thread(d1);
		t1.start();
		t1.join();
		System.out.println("Counters: " + a.get() + ", " + b.get());
	}
}</code></pre>
<p>So the output of this will be something like:</p>
<pre>Counters: 5, 10
Drain(Drain@3be5d207): 4, 11
Drain(Drain@3be5d207): 3, 12
Drain(Drain@3be5d207): 2, 13
Drain(Drain@3be5d207): 1, 14
Drain(Drain@3be5d207): 0, 15</pre>

<p>I think it's fair to say that this small program is actually reasonably correct, and could be used (or "composed") in a larger program and work reliably. But of course, what happens when we write a slightly different program, using two drains?</p>
</div>



<div class="slide">
<h1>Going down the multithreaded drain.</h1>
<p>So now let's write a program to use two drains:</p>
<pre><code class="java">public final class Threads2 {
	public static void main(String[] args) throws Exception {
		final Counter a = new Counter(5);
		final Counter b = new Counter(10);
		final Thread t1 = new Thread(new Drain(a, b));
		final Thread t2 = new Thread(new Drain(b, a));
		t1.start();
		t2.start();
		t1.join();
		t2.join();
		System.out.println("Counters: " + a.get() + ", " + b.get());
	}
}</code></pre>
<p>So what does this program output?</p>
<pre>Counters: 5, 10
Drain(Drain@1f2f0ce9): 4, 11</pre>
<p>Well, at least that's what it's output so far. I wrote this sometime last week, and I am still waiting for it to finish...</p>
</div>



<div class="slide">
<h1>Going down the multithreaded drain.</h1>
<p>So now let's write a program to use two drains:</p>
<pre><code class="java">public final class Threads2 {
	public static void main(String[] args) throws Exception {
		final Counter a = new Counter(5);
		final Counter b = new Counter(10);
		final Thread t1 = new Thread(new Drain(a, b));
		final Thread t2 = new Thread(new Drain(b, a));
		t1.start();
		t2.start();
		t1.join();
		t2.join();
		System.out.println("Counters: " + a.get() + ", " + b.get());
	}
}</code></pre>
<p>So what does this program output?</p>
<pre>Counters: 5, 10
Drain(Drain@1f2f0ce9): 4, 11</pre>
<p>Well, at least that's what it's output so far. I wrote this sometime last week, and I am still waiting for it to finish...</p>
<p>But of course, if you're paying attention, you'll realize that this program will never finish. Feel free to play with the sleep values though, because you can get other incorrect behaviors too (not always reliably incorrect of course...), like thread starvation, and so on. The issue lies in the <code>synchronized</code> blocks. Both drains lock the source first, then the sink. In this case, they each lock the counter the other one needs first, and then start waiting (for forever) until the other lock becomes available, of course, since the other drain holds the lock and is sleeping until the other one releases the lock, this can never happen. Now, I have gone to some effort to make an "obvious" and reliable deadlock (this is a textbook deadlock), but when people say that multithreaded programming is hard to do correctly, this is one example of what is discussed.</p>
</div>



<div class="slide">
<h1>A second act</h1>
<p>Now, this is a group of functional programmers, so clearly the mistake is that I used Java. So let's rewrite this in Erlang. So here's a counter and a drain implementation, along with two functions to start single and double drain versions:</p>
<pre><code class="erlang">-module(threads).
-export([counter/1, drain/2, startOneDrain/0, startTwoDrains/0]).

counter(X, Lock_PID) ->
	receive
		{ counter_set, NewX, Lock_PID } ->
			io:format("counter ~w: ~w~n", [ self(), NewX ]),
			counter(NewX, Lock_PID);
		{ counter_inc, Lock_PID } ->
			io:format("counter ~w: ~w~n", [ self(), X + 1 ]),
			Lock_PID ! { counter_inc, X + 1, self() },
			counter(X + 1, Lock_PID);
		{ counter_dec, Lock_PID } ->
			io:format("counter ~w: ~w~n", [ self(), X - 1 ]),
			Lock_PID ! { counter_dec, X - 1, self() },
			counter(X - 1, Lock_PID);
		{ counter_unlock, Lock_PID } ->
			io:format("counter ~w UNLOCKED ~w~n", [ self(), Lock_PID ]),
			counter(X);
		_ ->
			counter(X, Lock_PID)
	end.

counter(X) ->
	receive
		{ counter_lock, Lock_PID } ->
			io:format("counter ~w locking for ~w: ~w~n", [ self(), Lock_PID, X ]),
			Lock_PID ! { ok, X, self() },
			counter(X, Lock_PID);
		_ ->
			counter(X)
	end.

drain(done) ->
	io:format("Drain finished. ~w~n.", [ self() ]).

drain_sink_ok(Source, Sink) ->
	receive
		{ counter_inc, _, Sink } ->
			Sink ! { counter_unlock, self() },
			Source ! { counter_unlock, self() },
			drain(Source, Sink);
		_ ->
			drain_sink_ok(Source, Sink)
	end.

drain_wait_for_sink(Source, Sink, X) ->
	receive
		{ ok, Y, Sink } ->
			Source ! { counter_dec, self() },
			Sink ! { counter_inc, self() },
			io:format("Draining ~w: ~w to ~w~n", [self(), X, Y]),
			drain_sink_ok(Source, Sink);
		_ ->
			drain_wait_for_sink(Source, Sink, X)
	end.

drain(Source, Sink) ->
	Source ! { counter_lock, self() },
	receive
		{ ok, 0, Source } ->
			Source ! { counter_unlock, self() },
			drain(done);
		{ ok, X, Source } ->
			Sink ! { counter_lock, self() },
			drain_wait_for_sink(Source, Sink, X);
		_ ->
			drain(Source, Sink)
	end.

startOneDrain() ->
	A = spawn(threads, counter, [5]),
	B = spawn(threads, counter, [10]),
	spawn(threads, drain, [ A, B ]).

startTwoDrains() ->
	A = spawn(threads, counter, [5]),
	B = spawn(threads, counter, [10]),
	spawn(threads, drain, [ A, B ]),
	spawn(threads, drain, [ B, A ]).</code></pre>
</div>


<div class="slide">
	<h1>Erlang to the rescue!?</h1>
	<p>So when we start up the Erlang interpreter and compile our module (<code>erl</code> followed by <code>c(threads).</code>) and run the single drain version(do a <code>threads:startOneDrain().</code>), we get something like this:
<pre>1> c(threads).
{ok,threads}
2> threads:startOneDrain().
counter &lt;0.40.0> locking for &lt;0.42.0>: 5
&lt;0.42.0>
counter &lt;0.41.0> locking for &lt;0.42.0>: 10
Draining &lt;0.42.0>: 5 to 10
counter &lt;0.40.0>: 4
counter &lt;0.41.0>: 11
counter &lt;0.41.0> UNLOCKED &lt;0.42.0>
counter &lt;0.40.0> UNLOCKED &lt;0.42.0>
counter &lt;0.40.0> locking for &lt;0.42.0>: 4
counter &lt;0.41.0> locking for &lt;0.42.0>: 11
# omitted ...
Draining &lt;0.42.0>: 1 to 14
counter &lt;0.40.0>: 0
counter &lt;0.41.0>: 15
counter &lt;0.41.0> UNLOCKED &lt;0.42.0>
counter &lt;0.40.0> UNLOCKED &lt;0.42.0>
counter &lt;0.40.0> locking for &lt;0.42.0>: 0
Drain finished. &lt;0.42.0>
counter &lt;0.40.0> UNLOCKED &lt;0.42.0></pre>

<p>Great! So let's try the two drain version! <code>threads:startTwoDrains().</code>, and we get...</p>
<pre>3> threads:startTwoDrains().
counter &lt;0.45.0> locking for &lt;0.47.0>: 5
counter &lt;0.46.0> locking for &lt;0.48.0>: 10
&lt;0.48.0>
4></pre>
<p>Hey... wait a minute... (or 10 years... it's all the same). Erlang's superior actor concurrency, non-shared state, functional style did nothing to prevent me from duplicating the same deadlock bug my Java version had...</p>
</div>



<div class="slide">
<h1>Composing threads</h1>
<p>Multithreaded programs have an interesting property. We can write correct multithreaded programs, and then combine them with other multithreaded programs, that are also correct, and there is no guarantee that larger combined program will be correct. This goes way beyond mutable state. And while some programs assign numerical values to locks and use the ordering imposed by the integer values on locks to have a canonical locking order, there are still other bugs that regularly occur. Many result in security issues, for instance data races in authentication and e-commerce systems in nearly every major web service have lead to exploitable bugs that resulted in users being able to impersonate other users, double spend transactions, and much more.</p>

<p>I have a claim.</p>
<h1>Mulithreaded programming is hard because there is no composition operator that preserves correctness when combining two correct multithreaded programs.</h1>

<p>Actor concurrency, with "shared nothing" memory, is a great and useful thing, but even with it you can still write programs using actors that can not be combined without bugs(like deadlock, livelock, and friends).</p>
</div>


<div class="slide">
<h1>Question: So wait, that was an example of combining two "smaller" programs into a "bigger" program using functions?</h1>
</div>



<div class="slide">
<h1>Question: So wait, that was an example of combining two "smaller" programs into a "bigger" program using functions?</h1>
<h1>Answer: Yes, that was.</h1>
</div>



<div class="slide">
<h1>Question: So wait, that was an example of combining two "smaller" programs into a "bigger" program using functions?</h1>
<h1>Answer: Yes, that was.</h1>
<h1>Question: It still didn't work though?</h1>
</div>



<div class="slide">
<h1>Question: So wait, that was an example of combining two "smaller" programs into a "bigger" program using functions?</h1>
<h1>Answer: Yes, that was.</h1>
<h1>Question: It still didn't work though?</h1>
<h1>Answer:</h1>
<p>That's because our function that performed the composition lacked a useful property. It didn't "preserve correctness". Ideally, a composition operator(aka function) will preserve some "invariant". However, we can define different invariants that prove different things.</p>

<p>In fact, one of the insights functional programming has helped bring to the world was the idea that composition should preserve some type of "correctness property. Many research projects have been started to find operators that do preserve correctness. For instance, let's consider another way to write the previous example using <a href="https://bartoszmilewski.com/2010/09/11/beyond-locks-software-transactional-memory/">Software Transactional Memory</a></p>
</div>



<div class="slide">
<h1>Question: So wait, that was an example of combining two "smaller" programs into a "bigger" program using functions?</h1>
<h1>Answer: Yes, that was.</h1>
<h1>Question: It still didn't work though?</h1>
<h1>Answer:</h1>
<p>That's because our function that performed the composition lacked a useful property. It didn't "preserve correctness". Ideally, a composition operator(aka function) will preserve some "invariant". However, we can define different invariants that prove different things.</p>

<p>In fact, one of the insights functional programming has helpd bring to the world was the idea that composition should preserve some type of "correctness property. Many research projects have been started to find operators that do preserve correctness. For instance, let's consider another way to write the previous example using <a href="https://bartoszmilewski.com/2010/09/11/beyond-locks-software-transactional-memory/">Software Transactional Memory</a></p>
<h1>Question: My head is hurting again...</h1>
</div>

<div class="slide">
<h1>Question: So wait, that was an example of combining two "smaller" programs into a "bigger" program using functions?</h1>
<h1>Answer: Yes, that was.</h1>
<h1>Question: It still didn't work though?</h1>
<h1>Answer:</h1>
<p>That's because our function that performed the composition lacked a useful property. It didn't "preserve correctness". Ideally, a composition operator(aka function) will preserve some "invariant". However, we can define different invariants that prove different things.</p>

<p>In fact, one of the insights functional programming has helpd bring to the world was the idea that composition should preserve some type of "correctness property. Many research projects have been started to find operators that do preserve correctness. For instance, let's consider another way to write the previous example using <a href="https://bartoszmilewski.com/2010/09/11/beyond-locks-software-transactional-memory/">Software Transactional Memory</a></p>
<h1>Question: My head is hurting again...</h1>
<h1>Answer: Let's rollback...</h1>
</div>



<div class="slide">
<h1>Threads, this time with commitment.</h1>
<p>The idea of Software Transactional Memory(STM) was worked out in the early 2000's by Simon Peyton Jones, Tim Harris, Maurice Herlihy, and Simon Marlow and implemented as a library in Haskell, and then in many other languages. It works much like the idea of database transactions: a block of code is executed atomically, and like a transaction, it either commits or fails(and can be retried until it eventually succeeds). It is composable and eliminates data races, deadlocks, and other concurrency bugs. It is not without it's critics (and implementation difficulties, see this <a href="http://joeduffyblog.com/2010/01/03/a-brief-retrospective-on-transactional-memory/">piece</a> by Joe Duffy about STM in .Net), but it can greatly simplify concurrent programming.</p>

<p>In Haskell, STM is implemented in the <code>Control.Concurrent.STM</code> package, and consider this reimplementation of draining from one counter to another:</p>
<pre><code class="haskell">import Control.Concurrent
import Control.Concurrent.STM
import Control.Monad

moveOne 0 to source sink = return ()
moveOne from to source sink = do
    writeTVar source (from - 1)
    writeTVar sink (to + 1)

drainOne source sink = do
    from &lt;- readTVar source
    to &lt;- readTVar sink
    moveOne from to source sink

drain name source sink lock = do
    from &lt;- atomically $ readTVar source
    to &lt;- atomically $ readTVar sink
    withMVar lock $ \_ -> putStrLn $ "draining " ++ name ++ " " ++ (show from) ++ ", " ++ (show to)
    if from > 0
        then
            (atomically $ drainOne source sink) >> milliSleep 1 >> (drain name source sink lock)
        else return ()

milliSleep = threadDelay . (*) 1000</code></pre>
</div>



<div class="slide">
<h1>Software Transactional Memory drains</h1>
<p>For a single drain, this is the main:
<pre><code class="haskell">main = do
    lock &lt;- newMVar ()
    ctr1 &lt;- atomically $ newTVar 5
    ctr2 &lt;- atomically $ newTVar 10
    forkIO $ drain "a" ctr1 ctr2 lock
    milliSleep 60</code></pre>
And here is the output:
<pre>draining a 5, 10
draining a 4, 11
draining a 3, 12
draining a 2, 13
draining a 1, 14
draining a 0, 15</pre></p>
</div>

<div class="slide">
<h1>Software Transactional Memory drains</h1>
<p>Now two drains:
<pre><code class="haskell">main = do
    lock &lt;- newMVar ()
    ctr1 &lt;- atomically $ newTVar 5
    ctr2 &lt;- atomically $ newTVar 10
    forkIO $ drain "a" ctr1 ctr2 lock
    forkIO $ drain "b" ctr2 ctr1 lock
    milliSleep 60</code></pre>
And here is the output:
<pre>draining b 10, 5
draining a 5, 10
draining b 10, 5
draining a 5, 10
draining b 11, 4
draining a 5, 10
draining b 10, 5
draining a 5, 10
draining b 10, 5
draining b 10, 5
draining a 5, 10
draining b 10, 5
# many lines of output omitted
draining a 5, 10
draining b 10, 5
draining a 5, 10
draining a 5, 10
draining b 10, 5
draining a 5, 10</pre></p>

<p>So wait, this will just keep going like this no matter how long we delay? Each drain is trying to completely 0 the opposite counter, since they are both moving fairly quickly, and working against each other, neither is going to win (since now they aren't deadlocking and livelocking each other). However, I did have a lock in this, which you might argue is helping me(to be fair to me, locks didn't help me in the other two versions). I did have a version with no lock which behaved the same, except, since both threads were writing logging information to <code>stdout</code>, I got a jumbled mess of interleaved messages. I added the lock so that <code>putStrLn</code> would not jumble the status messages. After all, how else was I going to get output to paste into a slide? Feel free to remove the lock at home. Or, perhaps more fun, refactor the program to use STM to send logging messages to another thread that prints out the messages for us one at a time...</p>
</div>



<div class="slide">
<h1>Sugar free Software Transactional Memory drains</h1>
<p>The idea that we combine things with functions is something I want to emphasize. And that is exactly what the Haskell version of this problem did. However, Haskell actually has a couple of things that can obscure this. Namely, what is called "do-notation" is fancy syntax for making an anonymous function and passing it to a function named <code>>>=</code>. Now <code>>>=</code> is often termed an "operator" in Haskell, but operators are just two parameter functions that can be used "infix style". You can just call them like a normal function by wrapping them in parenthesis and passing arguments to them, like so: <code>(>>=) a b</code>. That would apply the function <code>>>=</code> to the parameters <code>a</code> and <code>b</code>. The fact that Haskell reserves some non-character symbols so it can distinguish "operators" from "normal function names", can obscure this if you're used to other languages. Also, anonymous functions are expressed like so: <code>(\param -> expression)</code>. The backslash is meant to look like a &lambda; because anonymous functions in the lambda calculus are denoted with &lambda;. One of the first steps the Haskell compiler does is "desugar" do-notation. So, without further ado, here is a "desugared" version of the exact same code from the previous page. So now you can see that: a) It's just function calls, b) Haskell is just a LISP in disguise<sup>[1]</sup>.</p>
<pre><code class="haskell">import Control.Concurrent
import Control.Concurrent.STM
import Control.Monad

moveOne from to source sink =
    (case ((==) 0 from) of
        True -> (return ())
        False ->
            ((>>=)
                (writeTVar source (from - 1))
                (\_ -> (writeTVar sink (to + 1)))))

drainOne source sink =
    ((>>=)
        (readTVar source)
        (\from -> ((>>=)
            (readTVar sink)
            (\to -> (moveOne from to source sink)))))

drain name source sink lock =
    ((>>=)
        (atomically (readTVar source))
        (\from ->
            ((>>=)
                (atomically (readTVar sink))
                (\to ->
                    ((>>=)
                        (withMVar lock (\_ -> putStrLn (((++) "draining " ((++) name ((++) " " ((++) (show from) ((++) ", " (show to)))))))))
                        (\_ -> (case ((>) from 0) of
                            True ->
                                ((>>=)
                                    (atomically (drainOne source sink))
                                    (\_ -> ((>>=)
                                        (milliSleep 1)
                                        (\_ -> (drain name source sink lock)))))
                            False -> (return ()))))))))


oneDrain lock ctr1 ctr2 = forkIO (drain "a" ctr1 ctr2 lock)

twoDrains lock ctr1 ctr2 =
    ((>>=)
        (forkIO (drain "a" ctr1 ctr2 lock))
        (\_ -> (forkIO (drain "b" ctr2 ctr1 lock))))

main =
    ((>>=)
        (newMVar ())
        (\lock ->
            ((>>=)
                (atomically (newTVar 5))
                (\ctr1 ->
                    ((>>=)
                        (atomically (newTVar 10)) 
                        (\ctr2 ->
                            ((>>=)
                                (oneDrain lock ctr1 ctr2)
                                --(twoDrains lock ctr1 ctr2)
                                (\_ -> (milliSleep 60)))))))))

milliSleep time = threadDelay ((*) 1000 time)</code></pre>
<p>[1] LISP = Lots of Infernal Stupid Parenthesis</p>
</div>


<div class="slide">
<h1>Question: Okay, so that STM stuff is neat. But I don't write a lot of parallel code.</h1>
</div>

<div class="slide">
<h1>Question: Okay, so that STM stuff is neat. But I don't write a lot of parallel code.</h1>
<h1>Answer: Composition shows up everywhere, let's look at some simple loops.</h1>
</div>


<div class="slide">
<h1>Question: Okay, so that STM stuff is neat. But I don't write a lot of parallel code.</h1>
<h1>Answer: Composition shows up everywhere, let's look at some simple loops.</h1>
<h1>Question: "Simple"? You keep using that word, I do not think it means what you think it means.</h1>
</div>



<div class="slide">
<h1>Loops</h1>
<p></p>

TO DO loop fusion map/fold/reduce examples
</div>



<div class="slide">
<h1>Arithmetic (Elementary Algebra)</h1>
<p>Most of us are familiar with numbers, and some basic operations on them like addition, subtraction, multiplication, and division. For better or worse, we even had some terminology inflicted on us in school. So let's review some of those terms and ideas. Please remember, the following slides are discussing properties of addition and multiplication and numbers. Don't try to over think it.</p>

<dl>
	<dt>Infix style: <code>a + 0 = 0 + a = a</code></dt>
	<dt>Prefix style: <code>+(a,0) = +(0,a) = a</code></dt>
	<dd>This is the <em>existence of an identity element</em> property. The idea is that there is an element such that when you combine it with anything else, you just get the same thing. For multiplication, the identity is of course 1 since <code>1 * a = a * 1 = a</code>. For addition, <code>0 + a = a + 0 = a</code>.</dd>

	<dt>Infix style: <code>a + a-inverse = a-inverse + a = 0</code></dt>
	<dt>Prefix style: <code>+(a, inverse(a)) = +(inverse(a), a) = a</code><dt>
	<dd>This is the <em>existence of inverses</em> property. The idea is that for every number or element in the set we have another number or element that acts as it's "inverse", such that when we combine them we get the identity element or number. One important idea that comes out of this is that subtraction and division can be thought of as "non-primitive" operations, and instead the addition (or multiplication) by an "inverse" operator that finds the inverse of an element or number. So <code>a - b = a + inverse(b) = a + b<sup>-1</sup></code>. The "superscripted -1" is often used as notation for "take the inverse" in algebra. Let's look at some examples to help explain: <code>a + -a = 0</code>, with multiplication <code>a * 1/a = 1</code>.</dd>
</dl>
</div>



<div class="slide">
<h1>Arithmetic (Elementary Algebra), part 2</h1>
<dl>
	<dt>Infix style: <code>a + (b + c) = (a + b) + c</code></dt>
	<dt>Prefix style: <code>+(a,+(b,c)) = +(+(a,b), c)</code></dt>
	<dd>This is the <em>associative property</em>. The idea is rather simple: it doesn't matter which pair of numbers we "add" together first, as long as we don't change the order(i.e. grouping doesn't matter). Of course, we might decide to confuse this with commutativity which says "order" doesn't matter, because addition and multiplication are commutative as well. For an example of an operation that is associative but not commutative consider list concatenation: <code>[1] + ([2] + [3]) = ([1] + [2]) + [3] = [1,2,3]</code>, but <code>[1] + [2] != [2] + [1]</code> (because <code>[1,2] != [2,1]</code>)</dd>

	<dt>Infix style: <code>a + b = b + a</code></dt>
	<dt>Prefix style: <code>+(a,b) = +(b,a)</code></dt>
	<dd>This is the <em>commutative property</em>. The idea is that it doesn't matter which parameter to the operation comes first, the result is the same either way. Since we're sticking to "numbers" let's talk about an operation which is commutative but not associative, consider the idea of taking the "midpoint" as a binary operation: <code>mid(a,b) = (a + b) / 2</code>. Now, <code>mid(a,b) = (a + b)/2 = (b + a)/2 = mid(b,a)</code>, so this is commutative, but consider: <code>mid(a,mid(b,c)) = (a + ((b+c)/2)) / 2 = a/2 + b/4 + c/4</code>, and <code>mid(mid(a,b),c) = (((a+b)/2) + c) / 2 = a/4 + b/4 + c/2</code>. So, let a = 2, b = 3, c = 4, and we have 2.75 and 3.25 respectively, so clearly, this is not associative.</dd>
</dl>
</div>



<div class="slide">
<h1>Arithmetic (Elementary Algebra), part 3</h1>
<dl>
	<dt>Infix style: <code>a * (b + c) = a*b + a*c</code></dt>
	<dt>Prefix style: <code>*(a,+(b,c)) = +(*(a,b),*(a,c))</code></dt>
	<dd>This is the <em>distributive property</em>. Previously we were just talking about one operation, the distributive property is about two operations. Of course. This actually isn't entirely true... considering we discussed the idea that some operations are NOT commutative, it's more fair to call this the <strong>left</strong> <em>distributive property</em>. Not every pair of operations is distributive as well, but I'll ignore that for the time being.</dd>

	<dt>Infix style: <code>(a + b) * c = a*c + b*c</code></dt>
	<dt>Prefix style: <code>*(+(a,b),c) = +(*(a,c),*(b,c))</code></dt>
	<dd>This is the <em>right distributive property</em>. For completeness, since not everything is commutative.</dd>

	<dt>Infix style: <code>a = b, b = c implies a = c</code></dt>
	<dt>Prefix style: <code>=(a,b) and =(b,c) implies =(a,c)</code></dt>
	<dd>This is the <em>transitive property</em>. Equality itself is a "binary operation", and one of the most important aspects of it is that it is transitive. But there are other transitive operations: "less than or equal to"(<code>&lt;=</code>) is transitive, so is "greater than or equal to". "is subset of" is also transitive. This idea becomes very important, we'll come back to it later.</dd>

	<dt><code>f(g(x)) = (f &compfn; g)(x)</code></dt>
	<dd>The &compfn; is known as the <em>function composition operator</em>. The previous discussions about these properties have focused on numbers and operations on numbers, but in algebra we talk about "operations" a lot, and function composition is an operation, it even has a symbol.</dd>
</dl>
</div>



<div class="slide">
<h1>Mathematical and Logic symbols</h1>
<p>While the previous discussion of elementary algebra should have been a "review" in some sense, this is probably new material for many people. When reading mathematical texts, especially ones involving logic, there are many symbols that are used in a specific, formal way. However, many of these symbols can be "read" as phrases/words in English that should make sense to many programmers.</p>

<p><strong>Note:</strong> This list is quite long and yet, it's not comprehensive. These symbols came from mathematical literature, and they also show up in a lot of functional programming literature, papers, and references. These symbols mostly have straightforward meanings, but the process of acclimating to their usage can take some time. It's a lot like reading a Perl program. However, unlike Perl, these concepts can be used to increase the clarity of your thinking.</p>

<dl>
	<dt><code>&forall;</code></dt>
	<dd>"Universal Quantification". When writing in English you can substitute the phrase <em>for all</em> or <em>for every</em>. It expresses the notion that some statemant is true "for every" value. While it is easy to compare this to "looping" in programming, it is probably more accurate to compare "for all" statements to "loop invariants", i.e. something that will always be "true".</dd>
	<dt><code>&Exists;</code></dt>
	<dd>"Existential Quantification". When writing this in English the phrase <em>there exists</em>, <em>there is</em>, or <em>there are</em> should be used. It can be thought of as implying a search for value that would be successful. There is a value that would satisfy some statement.</dd>
	<dt><code>&Element;</code></dt>
	<dd>When reading this symbol read it as "element of", "in".</dd>
	<dt><code>&top;</code></dt>
	<dd>This symbol is called "Top", think of it as a "penultimate maximum" value.</dd>
	<dt><code>&bot;</code></dt>
	<dd>This symbol is called "Bottom", think of it as a "penultimate minimum" value.</dd>
	<dt><code>&VerticalBar;</code> also <code>:</code></dt>
	<dd>This symbol is read as <em>such that</em> or <em>where</em>. It is used to place a condition or predicate (the right side) on the expression on the left side.</dd>
	<dt><code>&and;</code></dt>
	<dd>"Conjunction". <em>and</em>. Really, just a logical "and".</dd>
	<dt><code>&or;</code></dt>
	<dd>"Disjunction". <em>or</em>. Really, just a logical "or".</dd>
	<dt><code>&Implies;</code></dt>
	<dd>"Implication". When read this symbol use the word "implies" or "if - then". Basically, if the left hand side is true, then the right hand side is entailed or implied to be true as well.</dd>
	<dt>(a horizontal bar)</dt>
	<dd>"entails", "if/then", or "judgement". In logic, this is a way of expressing a conditional truth. Two expressions are written, one on top of the bar, the other on the bottom. If the top expression is true, then the bottom one is true as well, i.e. the bottom statement is entailed by the (truth of) the top statement.</dd>
</dl>
</div>



<div class="slide">
<h1>Sets</h1>
<p>In modern mathematics, sets have been formalized, but the idea is intuitive: a set of <em>things</em> has a notion of membership, a thing either is or is not a member of a set. There is no notion of order though. There is no "first" member of a set. However, we can frequently define sets (or subsets of sets) using some "predicate" to determine membership. For instance the "the set of all numbers evenly divisible by 2" is a set. Sets are often written as a list of things in between { and }, so <code>{ 1, 2, 3, 4 }</code> would be a set. The set with nothing in it is called the <em>empty set</em>, and it is often written: <code>&emptyset;</code>, however it can also be written <code>{}</code>. "Predicates" are mathematical statements whose values are either true or false. Sets that are defined in terms of another set and "predicates" are often written in a mathematical notation like so:
<code>Evens = { x &Element; &integers; &VerticalBar; x mod 2 = 0 }</code>.
Which if read in "English" says: <em>"The set 'Evens' is equal to the set x's in the Integers (&integers; is used to represent "the set of all integers") such that x modulo 2 is equal to 0."</em></p>

<h2>Operations on Sets</h2>
<dl>
	<dt><code>A &Union; B = { x &VerticalBar; x &Element; A or x &Element; B }</code></dt>
	<dd>This is the <em>set union</em> operator. It produces a new set. An element is a member of the new set if it was a member of at least one of the sets we combined.</dd>
	<dt><code>A &Intersection; B = { x &VerticalBar; x &Element; A and x &Element; B }</code></dt>
	<dd>This is the <em>set intersection</em> operator. It produces a new containing only elements that were in both sets.</dd>
	<dt><code>A &subset; B = true if and only if &forall; x &Element; A, x &Element; B</code></dt>
	<dd>The <em>subset</em> relationship is a binary operator like equality that is true if all the elements of A are also elements of B.</dd>
	<dt><code>A = B if A &subset; B and B &subset; A</code></dt>
	<dd>Two sets are considered equal if they are subsets of each other (i.e. every element of A is also in B, and every element in B is also in A).</dd>
	<dt><code>A &setminus; B = { x &Element; A &VerticalBar; x &notin; B }</code></dt>
	<dd><em>Set difference</em> or <em>complement</em> is an operator that gives a subset A containing only the elements that were not also in B. This could actually be the empty set (<code>&emptyset;</code>).</dd>
	<dt><code>A &Cross; B = { (a,b) &VerticalBar; a &Element; A, b &Element; B }</code></dt>
</dl>
</div>



<div class="slide">
<h1>Tuples</h1>
<p>A n-tuple is just an ordered "list" of <code>n</code> things. You can also have a set of tuples(in fact, tables in SQL are sets of n-tuples). A 2-tuple is often just called an "ordered pair". However, in ordinary mathematics, we like to reduce everything to sets, so I want to suggest we can encode "n-tuples" as sets. Now, the ordered pair: <code>(a,b)</code> can't simply be written as the set <code>{ a, b }</code>, because in a set order doesn't matter. But we can tell if two sets are equal to each other, we can also take the difference of two sets. So we can define a tuple(or "encode a tuple as a set") as follows:
<code>P = (a, b) = { {a}, {a,b} }</code>
And we can even define expressions in terms of set operations to give us the first and second elements.</p>

<dl>
	<dt><code>first(p) = &Union; &Intersection; p</code></dt>
	<dd>Breaking this down: the set of all x's such that for every element of the Y of the set P, x is an element of Y. Since the "first" element of the tuple will be in both sets, that is the only value that can satisfy this predicate.</dd>
	<dt><code>second(p) = &Union; { x &Element; &Union; p &VerticalBar; &Union; p &NotEqual; &Intersection; p &RightArrow; x &NotElement; &Intersection; p }</code></dt>
	<dd></dd>
</dl>

<p>This "technique" can be extended for any arbitrary "n"-tuples. In true mathematician style I will leave this as an exercise to the reader.</p>
</div>



<div class="slide">
<h1>Programming and sets</h1>
<p>Recall the subset notation previously. In Python we could write that as this code:
<pre><code class="python">Z = range(4294967295) # we'll pretend the maximum value for a 32-bit integer is "all integers"
Evens = [ x for x in Z if x % 2 == 0 ]</code></pre>
Those familiar with Python will recall this is called a "list comprehension". The general idea is to apply some bit of code to each element of a list to determine if the element is in the list.</p>

<p>In (PostgreSQL) SQL we could write:
<pre><code class="sql">SELECT x
    FROM (SELECT generate_series(0, 4294967295) AS x) Z
    WHERE x % 2 = 0;</code></pre>
Of course, tables and queries in SQL are sets of n-tuples(more on that soon), so this is probably not too surprising.</p>

<p>In Haskell we could write:
<pre><code class="haskell">Evens = [x | x &lt;- [1..], mod x 2 == 0 ]</code></pre></p>

<p>These examples were picked intentionally because mathematical notation, language, and concepts permeate many programming languages. But it goes even deeper than that.</p>
</div>



<div class="slide">
<h1>Functions and Relations</h1>
<p>Now that we know about sets and n-tuples, we can talk about <em>relations</em>. In mathematics, a relation is quite simply a set of n-tuples, where n is the same for every element of the set.</p>
<table>
<tr><td><code>(1, 2, 4)</code></td></tr>
<tr><td><code>(2, 4, 4)</code></td></tr>
<tr><td><code>(6, 4, 5)</code></td></tr>
<tr><td><code>(4, 2, 1)</code></td></tr>
<tr><td><code>(7, 9, 10)</code></td></tr>
</table>

<p>A <em>function</em> is a relation in which one element of the n-tuple is denoted the "key" and the rest of the tuple is denoted as the "value". The set of values that comprise the key's are called the <em>domain</em>, the set of the rest of the values from the n-tuple is called the <em>codomain</em>. Or to use another idea, the <em>domain</em> comes from one set, and the <em>codomain</em> comes from another set, and the relation/function is comprised of a subset of the cross product of the two sets.</p>

<h2>Combining Relations</h2>
<p>Now that we have some definitions, let's talk about combining relations (or functions). Given two relations that consist of ordered pairs, <code>F</code>(on the sets <code>Y &Cross; Z</code>) and <code>G</code>(on the sets <code>X &Cross; Y</code>), let's define a <code>combine</code>. Create a new set of ordered pairs, matching up the second element(using the <code>second</code> operation we defined on ordered pairs) of the pairs in G with the first element of the ordered pairs in F. Or:
<pre><code>combine(F,G) = { (x,z) &Element; X &Cross; Z &VerticalBar; &Exists; y &Element; Y : (x,y) &Element; G &and; (y,z) &Element; F }</code></pre></p>

<p>We can extend this to arbitrary n-tuples, but the idea is the same. In fact, we can realize that <code>combine</code> is simply the function composition operator we discussed earlier.</p>
</div>



<div class="slide">
<h1>Relational Algebra</h1>
<p>Many programmers have probably heard that SQL is based on something called the <em>relational algebra</em>. It's probably worth pointing out now that the mathematical definition of a <em>relation</em> is identical to the SQL definition. This is not an accident. In fact, SQL also has the function composition operator: <code>JOIN</code>:
<pre><code class="sql">SELECT x,z
&nbsp;&nbsp;&nbsp;&nbsp;FROM F
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;JOIN G
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ON (F.y = G.y);</code></pre>
(minor footnote: this only if the join condition is a simple equality... join is actually more flexible than function composition, of course, it is in fact a composition operator for creating new relations out of other relations... and that's what counts.)</p>

<p>Cross products in SQL are simply the <code>CROSS JOIN</code> operator. And of course, since relations are just sets of n-tuples, we have all the set operators available too, set unions(<code>UNION</code>):
<pre><code class="sql">(SELECT * FROM F) UNION (SELECT * FROM G)</code></pre></p>
<p>Set intersection(<code>INTERSECT</code>):
<pre><code class="sql">(SELECT * FROM F) INTERSECT (SELECT * FROM G)</code></pre></p>
<p>and set difference(<code>EXCEPT</code>):
<pre><code class="sql">(SELECT * FROM F) EXCEPT (SELECT * FROM G)</code></pre></p>
<p>We even have the <code>&Element;</code> notation in the form of <code>IN</code>:
<pre><code class="sql">SELECT * FROM F WHERE (x,y) IN (SELECT * FROM G);</code></pre></p>
</div>



<div class="slide">
<h1>Pure Functions</h1>
<p>Up to this point, I have been discussing <em>mathematical functions and relations</em>. However, programmers often deal with functions that involve some notion of "state", and thus, often have "functions" that have some internal state. A pure function is one that always maps the same parameters to the same values.</p>

<p>One reason to prefer stateless or pure functions is that state transitions cause a combinatorial "explosion" in terms of possibilities. In a system with 2 states, we have to consider the transitions from state 1 to 2, and 2 to 1. However, adding a single state to this, increases the possibilities to 6 (1 to 2, 1 to 3, 2 to 1, 2 to 3, 3 to 1, 3 to 2), and so on.</p>

<p>However, we can turn all functions with "state" into pure functions by simply adding a parameter whose value is the "state" of the system. And the value "returned" by the function will be the new state of the system. This transformation is called <em>world passing style</em>.</p>

<p>One interesting observation is that this turns a function of a single parameter into a binary operation. Since these functions are "pure": <code>op(a, b)</code> will always result in the same value. So now a sequence of the same operation can be written in the following way: <code>op(op(a,b), c)</code> where <code>b</code> and <code>c</code> are inputs, and <code>a</code> is initial state.</p>

<p>Considering that we are discussing algebraic operations, it would be interesting to contemplate whether or not these "world passing style" functions should be associative. In other words, should it be the case that: <code>op(op(a, b), c) = op(a, op(b, c))</code>. Or rather... what would it take to make it associative? (This is an exercise left for the reader, although we'll come back to this later.)</p>
</div>

<div class="slide">
<h1>Transitivity</h1>
TO DO
</div>



<div class="slide">
<h1>Abstract Algebra</h1>
<p>Elementary algebra looks at symbol manipulation involving operations over numbers. Abstract algebra is the branch of the mathematics that looks at generalized versions of these ideas over different structures, and identifies recurring patterns. From a programming perspective, this is a bit like "duck typing". The general forms(groups, rings, fields, lattices) are identified by the basic properties they have. Once an object is identified with these rules, one can reason about the abstract forms with little regard for the underlying specific instance.</p>

<p>Another way of stating this: Abstract algebra is rarely concerned with concrete examples of algebraic structures, but with the structures themselves. Because of this, it focuses on universal properties, and invariants, that apply to these structures, and universal ways of combining, and composing these structures.</p>
</div>



<div class="slide">
<h1>Abstract Algebra</h1>
<p>Elementary algebra looks at symbol manipulation involving operations over numbers. Abstract algebra is the branch of the mathematics that looks at generalized versions of these ideas over different structures, and identifies recurring patterns. From a programming perspective, this is a bit like "duck typing". The general forms(groups, rings, fields, lattices) are identified by the basic properties they have. Once an object is identified with these rules, one can reason about the abstract forms with little regard for the underlying specific instance.</p>

<p>Another way of stating this: Abstract algebra is rarely concerned with concrete examples of algebraic structures, but with the structures themselves. Because of this, it focuses on universal properties, and invariants, that apply to these structures, and universal ways of combining, and composing these structures.</p>
<h2>Composition of structures</h2>
<p>Abstract algebra provides a huge catalog of techniques for composing structures and universal properties about these structures. As we saw in the beginning, one way to approach building larger programs is by seeking out techniques of combining programs to create larger programs. In other words, such techniques could be termed an algebra of programs.</p>

<p>So let's talk about some basic algebraic structures.</p>
</div>



<div class="slide">
<h1>Semigroups</h1>
<p>A semigroup is about as simple as we can get algebraically. It consists of a set of "things" and a binary <strike>operator</strike> function over that set whose results are also members of that set, and the function must be associative.</p>
<p>No seriously, that's it. Nothing else.</p>
<p>Some people like to come up with metaphors, but the whole goal of abstract algebra is to get away from metaphors. A semigroup is any pair that has these properties, no more, no less.</p>
<p>So let's look at some things that are semigroups.</p>

<dl>
	<dt>Addition over the integers.</dt>
	<dd>Addition is associative: <code>2 + (3 + 4) = (2 + 3) + 4</code>. Addition of two integers always gives you another integer. Also, let's just write this as a programmer: <code>add(x, add(y, z)) = add(add(x,y), z)</code>. See, "prefix notation" and operators are just binary functions with a couple of rules.</dd>
	<dt>Multiplication over the integers.</dt>
	<dd>Multiplication is also associative, and multiplying two integers will result in an integer.</dd>
	<dt>Boolean Or over the booleans.</dt>
	<dd>Or is associative: <code>or(a, or(b,c)) = or(or(a,b), c)</code>(seriously, enumerate every possible option, it's possible in this case), and or'ing two booleans results in a boolean.</dd>
	<dt>Boolean And over the booleans.</dt>
	<dd>And is also associative, and and'ing two booleans gives us a boolean.</dd>
	<dt>List concatenation over lists.</dt>
	<dd>We already presented this, but let's hit it again: <code>concat(list1, list2)</code> will always give us another list, and <code>concat(list1, concat(list2, list3)) = concat(concat(list1, list2), list3)</code> is always true. Even I won't be so tedious as to present strings(a string is a list of characters. Right? If you don't believe me, go write some C code to handle Unicode strings as your punishment.)</dd>
	<dt>The "find the minimum of a pair" function, <code>min(a,b)</code>, over integers.</dt>
	<dd>So what is the min function? <code>min(a,b)</code> returns the smaller of the two values <code>a</code> and <code>b</code>. Now, this is an associative function: <code>min(a, min(b,c)) = min(min(a,b), c)</code>.</dd>
	<dt>The <code>max</code> function over the integers.</dt>
	<dd>The flip side of the min function: <code>max(a,b)</code> gives the larger of the two values, and it too is associative: <code>max(a, max(b,c)) = max(max(a,b), c)</code>.</dd>
	<dt>Set Union over, well, sets.</dt>
	<dd>Set union produces another set, and set union is associative: <code>union(a,union(b,c)) = union(union(a,b), c)</code>.</dd>
	<dt>Set Intersection over sets.</dt>
	<dd>Again, set intersection produces another set, and it is associative: <code>intersect(a,intersect(b,c)) = intersect(intersect(a,b),c)</code>.</dd>
	<dt>Finding the smallest bounding box that covers two bounding boxes.</dt>
	<dd>This is a bit of a quirky "small" example, but still fun to consider. Given two bounding boxes we can define a binary operation that returns a new minimal bounding box covering both boxes. It should be straightforward to see that this operation is associative.</dd>
	<dt>Function composition.... over pure functions whose domain and codomain match.</dt>
	<dd>Okay, that makes... hey wait a minute...</dd>
</dl>
</div>



<div class="slide">
<h1>Composition of pure functions</h1>
<p>First, let's look at function composition, and we'll do it in JavaScript to make it concrete:
<pre><code class="js">function compose(f,g) { return function(x) { return f(g(x)); }; }</code></pre>
Now, let's look at an impure function(which is why I have to do this in JavaScript, since every Haskell function is pure):
<pre><code class="js">function church(x) {
	return function (y) {
		x = x + 1;
		return y + x;
	};
}</code></pre></p>

<p>So <code>church</code> is a function that returns a function that also happens to capture some state. So let's compose it just to prove we can:
<pre><code class="js">let newFunction = compose(church(1), church(2));
console.log(newFunction(1)); // 6</code></pre>
Now, let's look at the associative property rule: <code>compose(f,compose(g,h)) = compose(compose(f,g),h)</code>. Okay, that's makes sense. So let's write two expressions and evaluate them and compare their results!
<pre><code class="js">
let evil = church(1);
let left = compose(evil, compose(church(2), evil));
let right = compose(compose(evil, church(2)), evil);
console.log(left(2)); // 10
console.log(right(2)); // 14</code></pre>
The evil function threaded state throughout our code, and thus, evil was evaluated 4 times, changing the value of x it used each time. And this is what happens when you don't keep church and state separate<sup>[1]</sup>.</p>

<p>So function composition of pure functions is almost associative... so what's this comment about the domain and codomain being the same? A semigroup is on a set, in this case a set of functions. The binary operator (in this case, function composition) must be defined for <em>every</em> pair of elements in the set, including: <code>compose(f,f) = g</code> for every <code>f</code> in the set, and <code>g</code> must also be in the set too. The easiest way to guarantee this is just say all the functions map the domain back to itself. That way the result of every function will be compatible with the parameter of every other function in the set.</p>

<p>[1] Jokes involving the lambda calculus are underappreciated in my opinion.</p>
</div>



<div class="slide">
<h1>Monoids</h1>
<p>A Monoid is pretty simple too. It consists of a set of "things" and a binary function over that set of things whose results are also members of that set, and the function must be associative. Also, there must be an element in that set that behaves as an identity with that operator.</p>
<p>If you're astute, you've realized that this is almost the same thing as a Semigroup. In fact, <em>every</em> Monoid is also a Semigroup. In fact, almost anything of interest that is a Semigroup is usually a Monoid. (And even when they aren't, Mathematicians pull some sleight of hand to fix that... more later.) So let's look at our list of pairs that are Semigroups!</p>

<ul>
	<li>Addition over the integers - <code>0</code> is the identity.</li>
	<li>Multiplication over the integers - <code>1</code> is the identity.</li>
	<li>Boolean or over the booleans - <code>false</code> is the identity.</li>
	<li>Boolean and over the booleans - <code>true</code> is the identity.</li>
	<li>List concatenation over lists - <code>[]</code>(The empty list) is the identity. <code>""</code>, likewise, the empty string for strings.</li>
	<li>The minimum function over the integers - Some would use <code>+&infin;</code>, and that is legitimate, but I prefer to use <code>&top;</code>, we'll come back to this on the next slide.</li>
	<li>The maximum function over the integers - <code>-&infin;</code> is the "obvious choice", but I prefer <code>&bot;</code>, but we'll discuss in a moment.</li>
	<li>Set union over sets - <code>&emptyset;</code>(the empty set) is the obvious choice.</li>
	<li>Set intersection overs sets - The set of all possible sets... wait... now things are completely breaking down. Let's say there is no obvious choice for the time being.</li>
	<li>Function composition over endomorphisms - The identity function is the correct choice. Also, we'll cover that word morphism later, but it's a function(morphism) whose codomain is equal to it's domain. Don't worry, function composition isn't the trick question on this slide.</li>
</ul>
</div>



<div class="slide">
<h1>Identity crisis.</h1>
<p>As we saw, most of our semigroups are also monoids because they have an "obvious" identity element. As we expanded to concepts like <code>min</code> and <code>max</code>, the obvious choice is problematic. Why is that? The concept of &infin; makes many mathematicians uncomfortable, and indeed, the rules often change in surprising way when discussing infinity<sup>[1]</sup>. Indeed, the concept of "positive infinity" and "negative infinity" are not properly considered part of the integers.</p>

<p>However, rather than try and add an infinite ordinal to the integers, and risk the wrath of the <a href="https://en.wikipedia.org/wiki/Axiom_of_choice">Axiom of Choice</a>, we can sidestep the entire issue by defining, ipso faco, two elements which are not an integer. Much like we often name "identity elements" <code>1</code> when discussing identity elements that aren't numbers(like the identity function), we often call these elements Top - <code>&top;</code>, and Bottom(or Bot) - <code>&bot;</code>.</p>

<p>Now, equipped with &top; and &bot; we can make the following statements about min and max:
<pre><code class="haskell">min(a, &top;) = min(&top;, a) = a
max(a, &bot;) = max(&bot;, a) = a</code></pre>
In fact, we can generalize this idea to any semigroup: create a new element that is not a member of the set of things and define the operation on the expanded set to satisfy the requirements that our element be an identity. However, finding an <em>interpretation</em> of this generated identity in the underlying concrete instance may be "problematic" to say the least.</p>

<p>For instance, in the case of set intersection, I have avoided being formal for the most part, but now, much as the case with pure functions that are endomorphisms, we must confront things with some clarification. Given a particular (finite, and in some cases infinite) set of items, we can construct a set that contains those items and use it as our identity with respect to set intersection, thus converting it into a monoid. However, the concept of a <em>set of all possible sets</em> is not "well defined", and indeed, is such a problematic question that we're going to come back to it in a few moments. But first...</p>

<p>[1] I spent 6 class sessions(3 weeks) of my Intro to Topology class standing at the blackboard trying to fix a proof involving the union of an infinite sequence of sets, while the entire class stood on watching... only later to discover that their lack of help was because they hadn't done their homework either, and were allowing the professor to spend class time on me so they could get caught up. Ben, if you're out there, I haven't forgotten...</p>
</div>



<div class="slide">
<h1>Lattices</h1>
<p>A lattice is an algebraic structure, like monoids, and semigroups, however, it is based on a binary relation that we will call a "partial order". To start with a simple example, &le;, the relation less-than-or-equal, on the integers is an ordering of the integers. In fact, lattices are a generalization of this concept of a sequence.</p>

<p>To be a partial order, a relation, called &le; here, must follow some rules:</p>
<code>a &le; a (reflexive)<br/>
if a &le; b and b &le; c then a &le; c (transitive)<br />
if a &le; b and b &le; a then a = b (antisymmetry)</code>

<p>In general, given a partial order, the top and bottom element of the set is denoted as "top"(&top;) and "bot"(&bot;). Informally this feels like positive and negative infinity, but this avoids the problems with infinity, and works well more abstract lattices.</p>

<h2>Types</h2>
<p>We can construct lattices of more than just numbers though. For instance, we can understand algebraic data types through the lens of lattices via the binary relation "is a subtype of", written as "<code>&lt;:</code>".</p>

<p>For instance, in many "object oriented" languages, <code>Object</code> is the supertype of every type, and thus, we can say for a type <em>T</em>, that <em>T</em> &lt;: <code>Object</code>. In languages like Haskell, there is a type called <code>Void</code> and it can be said to be a subtype of every type. (Haskell doesn't define a notion of a subtype in the language itself, however, we can say this speaking abstractly.)</p>
</div>



<div class="slide">
<h1>Cantor and the development of modern mathematics.</h1>
<p>Before we continue with more dense formalisms, I want to take a break to talk about the history of mathematics. Georg Cantor was a mathematician who lived between 1845 and 1918. If you've heard of the Cantor Set, this is the same Cantor (and that set has applications outside of pretty pictures of fractals, in particular, it's a useful example for measure theory in analysis).</p>

<p>In 1891, Cantor came out with a proof which set the world of Mathematics ablaze and arguments over his work helped develop modern mathematics. His proof was a deceptively simple discussion of the cardinality(read <strong>size</strong>) of infinite sets. In particular, the first part of his proof showed that the natural numbers(1, 2, 3, and so on), are the "same size" as the integers(0, 1, -1, 2, -2, and so on). Now, we would say he made a "one to one and onto mapping" between these sets. He also showed that the rational numbers(ratios of integers) were the same size. So far, everyone was with him, however, when we he got to the Real numbers(which include the irrationals, like &pi; and the &radic;2 and so on, which, is itself difficult tricky to define), he proved that there is no such mapping. That one can say that there are infinitely more "real numbers" than rationals, the "size" of the reals is a bigger infinity than the infinitely large size of the rationals.</p>

<p>This immediately started a <a href="https://en.wikipedia.org/wiki/Controversy_over_Cantor%27s_theory">storm of controversy</a> about the validity of his proof, and the counter-intuitive observation that there could be different "sizes" of infinity, and lead to such important question as "what exactly IS a proof anyway?".</p>

<h2>Russell's Paradox</h2>
<p>One of the people in this maelstrom was Bertrand Russell, who set out to formalize all of mathematics. This work was called <a href="https://en.wikipedia.org/wiki/Principia_Mathematica">Principia Mathematica</a> and it was during the development of this work, in 1901. that he enountered what we call <a href="http://plato.stanford.edu/entries/russell-paradox/">Russell's Paradox</a>, which is about the nature of "the set of all possible sets". Russell's solution was to develop a system of types of sets, where each "higher order" of set could only contain members of at most, the previous order. This was one of the first modern type systems and has connections to modern works on type theory.</p>

<h2>G&ouml;del's Incompleteness Theorem</h2>
<p>Bertrand Russell is often held up as a great figure in the history of mathematics. One example of why he is looked up favorably involves the work of Kurt G&ouml;del. G&ouml;del was studying formal systems, and in brief, his work was to show than "sufficiently powerful formal system"(i.e. one that can handle what we would call "arithmetic") cannot be "complete": that is, categorize every statement that can be expressed in that formal system as either true, or false. This leads to "undecidable" statements. His work on the <a href="http://plato.stanford.edu/entries/goedel/">Incompleteness Theorem</a> and it's related works, all done in the 1930's, is one of the greatest works of 20th century mathematics. It also rendered the project behind Russell's Principia Mathematica, and David Hilbert's program to formalize all of math pointless in some sense. And when G&ouml;del pointed out some of the implications in his correspondance with Russell, by all accounts, Russell took it quite well, and encouraged him in his work.</p>

<h2>Church, Turing, and the rest</h2>
<p>This project of formalizing mathematics, partially prompted by Cantor's proof, and answering questions about the nature of formal systems is directly connected to the work of Church and Turing on the nature of computability. Indeed, a theory of computation is one way to think about formal systems in general. Personally, I consider programming and computing branches of mathematics and mathematical logic(which seeks to answer questions about the formal systems underlying mathematics as a whole). The systems we use daily owe much to this revolution in mathematics at the turn of the twentieth century.</p>

<p>It's worth noting that Cantor's Diagonalization technique, one of the sources of this controversy is useful in proving the Halting Problem's undecidability, and G&ouml;del's Incompleteness theorems too.</p>
</div>



<div class="slide">
<h1>An Algebra of Types</h1>
TO DO
</div>



<div class="slide">
<h1>Morphisms</h1>
TO DO
</div>



<div class="slide">
<h1>Fixed points</h1>
<p>In the interest of providing an overview of concepts and demystifiying them, I want to briefly discuss fixed points. A longer discussion is quite worthwhile, but something to explore on your own, or discuss in a talk dedicated to the topic.</p>

<p>A fixed point of a function, <code>f</code>, is a value <code>x</code>, such that <code>f(x) = x</code>.</p>

<p>Now, it's probably easy to see that <em>every</em> value of the domain of the identity function is a fixed point of the identity function. In fact, that's the definition of the identity of function: <code>identity(x) = x</code>. Not every function has a fixed point, some have many fixed points.</p>

<p>However, this is all a discussion in terms of numbers. What about higher ordered functions? <code>f(x) = x</code> where the value of <code>x</code> is a function rather than a "simple" value?</p>

<p>It turns out that in the lambda calculus, there is a function that satisifies this "fixed point" property, and that function has been given the special name the <strong>Y Combinator</strong>. Even more fascinating, rather than being a mathematical curiousity, it turns out that the Y combinator is useful in definining recursive functions<sup>[1]</sup> like:
<pre><code class="scheme">(define (fib n)
	(if (&lt;= n 2)
		1
		(+ (fib (- n 1)) (fib (- n 2)))))</code></pre>
The problem that the Y Combinator solves is that <code>fib</code> is used inside the definition of <code>fib</code>. Similiarly, in algebraic data types:
<pre><code class="haskell">data Tree a = EmptyTree | Node a (Tree a) (Tree a)</code></pre>
<code>Tree</code> is used in the definition of <code>Tree</code>. Basically, the Y Combinator is used to define the formal semantics of these constructions behind the scenes. Another way of stating it: it allows anonymous functions to express self-reference(i.e. achieve recursion) even though we don't have "names". (One should also note, self reference is one consequence of Turing Completeness, in other words, <em>every</em> Turing Complete language has a fixed point function of some sort.)</p>

<p>So, fixed points of higher order functions are interesting, and a worthwhile thing to study, but are not often needed directly in day to day coding. (Or rather, if it is something you find yourself doing in "regular code", much like hand performing a translation of a program to continuation passing style, it is a sign that you should consider ask questions about the nature of the code you're writing and whether there is a way to exploit the language you are using to perform this sort of operation for you.) Mostly I am not trying to discourage this, but rather point out that going through a discussion like <a href="https://www.dreamsongs.com/Files/WhyOfY.pdf">The Why of Y</a> will take a couple of hours, a whiteboard, and possibly a mutiny of you convince a bunch of undergraduate Computer Science students that it's a fun way to spend a Thursday evening<sup>[2]</sup>.</p>

<p>[1] The startup accelerator Y Combinator is a company that bootstraps companies, and it's name is a play on the fact they are a "recursive" company. Yes, this is another instance of lambda calculus derived humor.</p>
<p>[2] I may or may not have done this in college.<sup>[3]</sup></p>
<p>[3] This talk may or may not be an example of me not learning my lesson the first time.<sup>[4]</sup></p>
<p>[4] Recursive footnotes are recursive.<sup>[4]</sup></p>
</div>



<div class="slide">
<h1>Category Theory</h1>

<p>Category Theory is often brought up as a bogeyman. So first, I want to bring up some historical context. It was originally developed in the 1940's and 1950's by Saunders Mac Lane in the study of Algebraic Topology. Topology is itself considered an abstract field devoted to studying connectivity in shapes and surfaces in a very general sense, asking about whether two things are the "same" in an abstract, stretchy, deformable way. Algebraic topology classifies topological spaces and asks questions about mapping families of topological spaces into algebraic concepts like groups, monoids and so on...</p>

<h2>Wat?</h2>
<p>In the words of my advisor, who was himself an Algebraic topologist, Category Theory is <em>Generalized Abstract Nonsense</em>.</p>

<h2>And yet...</h2>
<p>Classical mathematics uses classical logic and axioms on sets to describe and model every thing else in mathematics. Sets themselves are left undefined, and instead, stated as an entity that satisifes a collection of basic facts(called axioms). This is why defining functions, tuples, topologies can all be mapped down to set theory.</p>

<p>However, this is not the only way to do math. Category theory can be considered as an alternative foundation to mathematics that is not based on set-theoretic foundations leaving sets as the the undefined entity, and instead builds models of mathematical ideas in terms of functions rather than sets. In many textbooks, one of the first categories introduced is the category <code>Set</code>, which of course, is defined in terms of functions...</p>

<p>... and as programmers, we talk about programs in terms of functions. Building models of reasoning about programs in terms of Category Theory turns out to be a natural fit in many cases. Also, as a detour from modern classical mathematics (which itself only dates back to the work of Cantor, Russell, Frege, G&ouml;del, Turing, Church, and others around the turn of the 20th century), Category Theory is popular among Mathematicians who look at non-classical logics(like Intuitionistic logic) and constructivist approaches to mathematical logic.</p>

<p>One reason Category Theory is popular within computing is that it seeks to find universal properties that underly families of mathematical objects, even ones that are seemingly unrelated. This leads to another reason Category Theory is popular for some computer scientists is that it is very concerned with composition. The composition of functions, and of mathematical ideas in general. Ideas like composable mathematical proofs are popular to explore as well. Finding universal principles that apply to many "categorical objects" is also done. These ideas of finding invariants and ways to compose things have been at the heart of what I was trying to demonstrate with this talk: <em>How to build larger things from smaller things in ways that preserves the correctness/maintains the invariants, of the smaller things.</em>.</p>
</div>



<div class="slide">
<h1>Monads</h1>

<p>My talk has been on ideas from Abstract Algebra. Many of these ideas can (and are) reexpressed in terms of Category Theory to explain concepts from category theory like Monads: which as the (humorous) quote from the second slide observed are merely <em>a Monoid in the Category of the Endofunctors</em>. Even without exploring Category theory, you should now be able to see that this is probably talking about a monoid: an operator on a collection of items that is associative, and has an identity element from the collection, and that it the collection of objects, and that the objects are a higher order function that preserves identity and composition of functions, and the higher order function has the same domain and codomain.</p>

<h2>Monad Laws</h2>
<p>On the Haskell Wiki, you will see that Monad's must meet the requirements of the <a href="https://wiki.haskell.org/Monad_laws">Monad Laws</a>. One reason this matters is because Haskell's type system is not capable of proving that the functions involved follow those rules. However, if you scroll down to the bottom of that page, you'll see the laws recast in terms of a function called <code>>=></code>(the Kleisli composition operator), and a function called <code>return</code>:
<pre><code class="haskell">return >=> g &equiv; g
f >=> return &equiv; f
(f >=> g) >=> h &equiv; f >=> (g >=> h)</code></pre>
Which, looks a lot like the definition of a monoid... mostly because it is the definition of a monoid.</p>

<p>This talk was not meant to be a secret "monad tutorial". In fact, I don't want you to take away how to use or define monads. Instead...</p>
</div>



<div class="slide">
<h1>Monads</h1>

<p>My talk has been on ideas from Abstract Algebra. Many of these ideas can (and are) reexpressed in terms of Category Theory to explain concepts from category theory like Monads: which as the (humorous) quote from the second slide observed are merely <em>a Monoid in the Category of the Endofunctors</em>. Even without exploring Category theory, you should now be able to see that this is probably talking about a monoid: an operator on a collection of items that is associative, and has an identity element from the collection, and that it the collection of objects, and that the objects are a higher order function that preserves identity and composition of functions, and the higher order function has the same domain and codomain.</p>

<h2>Monad Laws</h2>
<p>On the Haskell Wiki, you will see that Monad's must meet the requirements of the <a href="https://wiki.haskell.org/Monad_laws">Monad Laws</a>. One reason this matters is because Haskell's type system is not capable of proving that the functions involved follow those rules. However, if you scroll down to the bottom of that page, you'll see the laws recast in terms of a function called <code>>=></code>(the Kleisli composition operator), and a function called <code>return</code>:
<pre><code class="haskell">return >=> g &equiv; g
f >=> return &equiv; f
(f >=> g) >=> h &equiv; f >=> (g >=> h)</code></pre>
Which, looks a lot like the definition of a monoid... mostly because it is the definition of a monoid.</p>

<p>This talk was not meant to be a secret "monad tutorial". In fact, I don't want you to take away how to use or define monads. Instead...</p>

<h2>Monads are about composition operators that preserve structure.</h2>
<p>In other words, when we define operators for composing things that preserve invariants or maintain some idea about correctness, it might be a monad. So it's yet another tool for talking about ways to compose things out of other things. I want to point out something important: <strong>Monads have nothing to do with side effects or I/O. Monads are a mathematical object that just "is".</strong> Now, because they are associative, they provide a framework for reasoning about things that require a degree of ordering and work with pure functions. Of course, Mathematically, impure functions don't exist (go back to the slide about "world passing style"...), so this is a bit of a redundant statement. But in a language like Haskell which only has pure functions(the first rule of <code>unsafePerformIO</code> is that we do not talk about <code>unsafePerformIO</code>) this turns out to be handy since, you know, side effects are useful, but it goes beyond that, since Monads are used as one (of several) general structuring principle in Haskell.</p>
</div>



<div class="slide">
<h1>A personal opinion</h1>
<p>I like category theory just fine, and it really is applicable to problems in computing. I think, however, that it gets overemphasized in some circles, and it's easy to latch on to it's terminology. You can use <em>algebraic</em> reasoning about code without resorting to category theory(although to be fair, a Category Theorist would state taht algebraic resasoning is the essence of Category Theory). Studying Category Theory will, in the longer run, provide insights into universal computational structures; however, if it's discouraging you from exploring functional programming, do not let it hold you back. In fact, it would probably make more sense to study the topic <em>after</em> you have explored functional programming for a while.</p>

<p>In fact, Monads are not even the only compositionally correct mathematical tool for reasoning about side effects<sup>[1]</sup>. Uniqueness types(also called linear types) are another popular approach to reason about side effects. They are not as universal as monads, but this also makes them a lot simpler. Of course, I suspect many of you are already at least aware of uniqueness typing: it's part of the theoretical basis for Rust's Ownership types. So, by all means, study category theory, unless you find yourself overwhelmed and discouraged from exploring functional programming, then, do not worry about it. There's always time for it later, and there are quite a few things to explore that will make it easier return to later.</p>

<p>[1] For criticisms of monads from respectable computer scientists who know their stuff, consider these two links: <a href="https://dl.dropboxusercontent.com/u/40457956/haskell_sucks.pdf">Haskell Sucks!</a> by Paul Bone and <a href="http://fexpr.blogspot.com/2011/12/trouble-with-monads.html">The trouble with monads</a> by John Shutt.</p>
</div>


<div class="slide">
<h1>Can Programming Be Liberated from the von Neumann Style? A Functional Style and Its Algebra of Programs.</h1>
<p>In 1977 (almost 40 years ago), the ACM awarded John Backus(yes, the "B" in Backus-Naur Form(BNF), the John Backus who helped INVENT and define Fortran AND Algol. As part of that award he delivered a lecture and wrote an article. The title of that article was <em>Can Programming Be Liberated from the von Neumann Style? A Functional Style and Its Algebra of Programs.</em></p>

<p>One of Backus's chief points was that in order to grow what we can do with software, we must work on languages for expressing software that make these larger programs possible to build, and the limitations the models of programming languages at the time were suffering from. Now, some of his complaints about obesity are a bit quaint considering that every programming language standard in 1977 that he was discussing is positively tiny compared to modern day languages(C++14 anyone? .Net or Java, take your pick.) But this is digression. One of the points his paper builds to is that in order to reason about larger programs, we must build them from smaller programs combined with algebraic primitives, that have useful mathematical properties that enable that reasoning. In other words, he wanted an algebra of programs with programming languages built on these concepts.</p>

<p>Below, I present a list of the section headings from this paper(emphasis mine), now keep in mind, this was 1977. Who said computer science moves at an impossible speed to keep up with? If you believe that, I challenge you to read this paper to challenge your thinking. It's quite readable even today, other than some drift in notation over the years.</p>

<ol>
	<li>Conventional Programming Languages: Fat and Flabby</li>
	<li>Models of Computing Systems
		<ol>
			<li>Criteria for Models
				<ol>
					<li>Foundations</li>
					<li>History Sensitivity</li>
					<li>Type of Semantics</li>
					<li><strong>Clarity and conceptual usefulness of programs</strong></li>
				</ol>
			</li>
			<li>Classification of Models
				<ol>
					<li>Simple operational models</li>
					<li><strong>Applicative models</strong></li>
					<li>Von Neumann models</li>
				</ol>
			</li>
		</ol>
	</li>
	<li>Von Neumann Computers</li>
	<li>Von Neumann Languages</li>
	<li>Comparison of von Neumann and Functional Programs
		<ol>
			<li>A von Neumann Program for Inner Product</li>
			<li>A Functional Program for Inner Product</li>
		</ol>
	</li>
	<li>Language Frameworks versus Changeable Parts</li>
	<li><strong>Changeable Parts and Combining Forms</strong></li>
	<li>APL versus Word-at-a-Time Programming</li>
	<li><strong>Von Neumann Languages Lack Useful Mathematical Properties</strong></li>
	<li>What Are the Alternatives to von Neumann Languages?</li>
	<li>Functional Programming Systems (FP Systems)
		<ol>
			<li>Introduction</li>
			<li>Description
				<ol>
					<li>Objects</li>
					<li><strong>Application</strong></li>
					<li><strong>Functions</strong></li>
					<li><strong>Functional forms</strong></li>
					<li>Definitions</li>
				</ol>
			</li>
			<li>Examples of Functional Programs
				<ol>
					<li><strong>Factorial</strong><em>(Editorial note from Emily: Some things never change...)</em></li>
					<li>Inner product</li>
					<li>Matrix multiply</li>
				</ol>
			</li>
			<li>Remarks about FP Systems
				<ol>
					<li>FP systems as programming languages</li>
					<li>Limitations of FP systems. <em>(Editorial note from Emily: He discusses the issue of modeling state in a pure language.)</em></li>
					<li>Expressive power of FP systems</li>
					<li>Advantages of FP systems</li>
				</ol>
			</li>
		</ol>
	</li>
	<li><strong>The Algebra of Programs for FP Systems</strong>
		<ol>
			<li>Introduction</li>
			<li><strong>Some Laws of the Algebra of Programs</strong></li>
			<li>Example: Equivalence of Two Matrix Multiplication Programs</li>
			<li>Expansion Theorems
				<ol>
					<li>Expansion</li>
					<li>Linear Expansion</li>
				</ol>
			</li>
			<li>A Recursion Theorem
				<ol>
					<li>Example: correctness proof of a reeursive factorial function.</li>
				</ol>
			</li>
			<li>An Iteration Theorem
				<ol>
					<li>Example: Correctness proof for an iterative factorial function.</li>
					<li>Example: proof of equivalence of two iterative programs.</li>
				</ol>
			</li>
			<li>Nonlinear Equations
				<ol>
					<li>Example: proof of idempotency</li>
				</ol>
			</li>
			<li><strong>Foundations for the Algebra of Programs</strong></li>
			<li><strong>The Algebra of Programs for the Lambda Calculus and for Combinators</strong></li>
			<li>Remarks</li>
		</ol>
	</li>
	<li><strong>Formal Systems for Functional Programming(FFP Systems)</strong>
		<ol>
			<li>Introduction</li>
			<li>Syntax</li>
			<li>Informal Remarks About FFP Semantics
				<ol>
					<li>The meaning of expressions; the semantic function &mu;</li>
					<li>How objects represent functions; the representation function &rho;</li>
					<li>Summary of the properties of &rho; and &mu;</li>
					<li>Cells, fetching, and storing</li>
				</ol>
			</li>
			<li>Formal Semantics for FFP Systems</li>
		</ol>
	</li>
	<li><strong>Applicative State Transition Systems (AST Systems)</strong> <em>(Editorial note from Emily: I find elements of this section rhyme with "Erlang")</em>
		<ol>
			<li>Introduction</li>
			<li>The Structure of Algol Compared to That of AST Systems</li>
			<li>Structure of an AST System
				<ol>
					<li>Transition rules for an elementary AST system</li>
					<li>Transition rules: exception conditions and startup.</li>
					<li>Program access to the state; the function &rho; DEFS</li>
				</ol>
			</li>
			<li>An Example of a System Program
				<ol>
					<li>General plan of the system program</li>
					<li>The "subsystem" function</li>
					<li>Installing the system program</li>
					<li>Using the system</li>
				</ol>
			</li>
			<li>Variants of AST Systems</li>
			<li>Remarks About AST Systems</li>
			<li>Naming Systems in AST and von Neumann Models</li>
		</ol>
	</li>
	<li>Remarks About Computer Design</li>
	<li>Summary</li>
</ol>
</div>

<div>
Morphisms
Maps
Bijective
Injective
Equivalence Classes
Domains

http://www.stephendiehl.com/posts/abstraction.html?HN_20160519

https://en.wikipedia.org/wiki/Morphism
https://en.wikipedia.org/wiki/Group_homomorphism
https://en.wikipedia.org/wiki/Map_(mathematics)
https://simple.wikipedia.org/wiki/Relation_(mathematics)
http://www.regentsprep.org/regents/math/algtrig/atp5/lfunction.htm
https://en.wikipedia.org/wiki/Ordered_pair

https://en.wikipedia.org/wiki/Monoid
https://en.wikipedia.org/wiki/Commutative_property#Noncommutative_operations_in_mathematics
https://en.wikipedia.org/wiki/Group_(mathematics)
https://en.wikipedia.org/wiki/Ring_(mathematics)
http://mathworld.wolfram.com/EquivalenceClass.html


https://en.wikipedia.org/wiki/Intuitionistic_type_theory
https://en.wikipedia.org/wiki/Homotopy_type_theory
https://homotopytypetheory.org/blog/
</div>



<div class="slide">
<h1>References</h1>
<ul>
	<li><a href="http://worrydream.com/refs/Backus-CanProgrammingBeLiberated.pdf">Can Programming Be Liberated from the von Neumann Style? A Functional Style and Its Algebra of Programs.</a> John Backus, 1977, Turing Award.</li>
	<li><a href="https://www.amazon.com/Mock-Mockingbird-Other-Logic-Puzzles/dp/0192801422">To Mock a Mockingbird: And Other Logic Puzzles</a></li>
	<li><a href="http://combinators.info/">Kestrels, Quirky Birds, and Hopeless Egocentricity</a></li>
	<li><a href="https://www.dreamsongs.com/Files/WhyOfY.pdf">The Why of Y</a></li>
	<li><a href="https://www.amazon.com/Little-Schemer-Daniel-P-Friedman/dp/0262560992">The Little Schemer</a></li>
	<li><a href="http://www.catonmat.net/blog/derivation-of-ycombinator/">Derivation of the Y Combinator</a></li>
</ul>

<h2>Software Engineering</h2>
<ul>
	<li><a href="https://www.youtube.com/watch?v=lLnsi522LS8">GOTO 2015 - Progress Toward an Engineering Discipline of Software - Mary Shaw</a>[video]</li>
	<li><a href="https://www.cs.umd.edu/class/spring2003/cmsc838p/Design/criteria.pdf">On the criteria to be used in decomposing systems into modules</a>, D. L. Parnas, 1979.</li>
</ul>

<h2>Concurrency and Parallelism</h2>
<ul>
	<li><a href="https://bartoszmilewski.com/2010/09/11/beyond-locks-software-transactional-memory/">Beyond Locks: Software Transactional Memory</a></li>
	<li><a href="http://research.microsoft.com/en-us/um/people/simonpj/papers/stm/stm.pdf">Composable Memory Transactions</a></li>
	<li><a href="http://www.usingcsp.com/cspbook.pdf">Communicating Sequential Processes</a>, C.A.R. Hoare</a></li>
	<li><a href="http://yosefk.com/blog/parallelism-and-concurrency-need-different-tools.html">Parallelism and concurrency need different tools</a></li>
	<li><a href="http://joeduffyblog.com/2010/01/03/a-brief-retrospective-on-transactional-memory/">A (brief) retrospective on transactional memory</a>, Joe Duffy</li>
</ul>
<h2>Type Theory</h2>
<ul>
	<li><a href="https://www.cis.upenn.edu/~bcpierce/tapl/">Types and Programming Languages</a>, Benjamin Pierce</li>
	<li><a href="http://chris-taylor.github.io/blog/2013/02/10/the-algebra-of-algebraic-data-types/">The Algebra of Algebraic Data Types, Part 1</a>, <a href="http://chris-taylor.github.io/blog/2013/02/11/the-algebra-of-algebraic-data-types-part-ii/">Part 2</a>, and <a href="http://chris-taylor.github.io/blog/2013/02/13/the-algebra-of-algebraic-data-types-part-iii/">Part 3</a>.</li>
</ul>

<h2>Category Theory</h2>
<ul>
	<li><a href="https://mitpress.mit.edu/books/basic-category-theory-computer-scientists">Basic Category Theory for Computer Scientists</a>, Benjamin Pierce</li>
	<li><a href="http://plato.stanford.edu/entries/category-theory/">Stanford Encyclopedia of Philosophy - Category Theory</a></li>
	<li><a href="http://www.stephendiehl.com/posts/monads.html">Monads Made Difficult</a></li>
	<li><a href="https://wiki.haskell.org/Monad_laws">Monad Laws</a></li>
	<li><a href="http://okmij.org/ftp/Computation/monadic-shell.html">Monadic i/o and UNIX shell programming</a></li>
	<li><a href="https://dl.dropboxusercontent.com/u/40457956/haskell_sucks.pdf">Haskell Sucks!</a>, Paul Bone</li>
	<li><a href="http://fexpr.blogspot.com/2011/12/trouble-with-monads.html">The trouble with monads</a>, John Shutt</li>
	<li><a href="http://comonad.com/reader/2009/recursion-schemes/">Recursion Schemes: A Field Guide</a></li>
</ul>

<h2>Haskell</h2>
<ul>
	<li><a href="https://wiki.haskell.org/Do_notation_considered_harmful">Do Notation Considered Harmful</a></li>
	<li><a href="http://www.haskellforall.com/2014/10/how-to-desugar-haskell-code.html">How to desugar Haskell code</a></li>
</ul>
</div>
</body>
</html>
