<!DOCTYPE html>
<html>
<head>
	<title>Algebra for Programmers</title>
	<link rel="stylesheet" href="default.min.css">
	<style type="text/css">
		h1 { text-align: center; }
		h2 { text-align: center; }
		h3 { text-align: center; }
		dd {
			margin-bottom: 1em;
			margin-top: 0.5em;
		}
	</style>
	<script src="highlight.min.js"></script>
	<script src="jquery-1.8.2.min.js"></script>
	<script src="slideshow.js"></script>
	<script>hljs.initHighlightingOnLoad();</script>
	<script>
		$(document).ready(function () {
			slideshow();
		});
</script>
</head>
<body>
<div class="slide">
	<h1>Algebra for Programmers</h1>
	<h2>Emily Estes</h2>
	<h3>2016-??-?? @ Functional Knox Meetup</h3>
	<p>To navigate: left arrow - previous slide, right arrow - next slide, escape - toggle slide show mode.</p>
</div>


<div class="slide">
<h1>Goals of this presentation</h1>
<ul>
	<li>Explain terminology frequently used in discussion of functional programming, in particular terms from algebra and (some) terms of category theory.</li>
	<li>Provide a review of topics in elementary algebra(i.e. what is typically covered in high school algebra classes).</li>
	<li>Connect algebraic topics to their appearance in functional programming.</li>
	<li>Demonstrate that building programs out of smaller programs using reliable combining operations is "doing algebra". (Or rather, algebra studies primitives that can help us study how to do this better.)</li>
	<li>Try to help alleviate "jargon overload" to people new to functional programming...</li>
</ul>
<blockquote>1990 - A committee formed by Simon Peyton-Jones, Paul Hudak, Philip Wadler, Ashton Kutcher, and People for the Ethical Treatment of Animals creates Haskell, a pure, non-strict, functional language. Haskell gets some resistance due to the complexity of using monads to control side effects. Wadler tries to appease critics by explaining that <em>"a monad is a monoid in the category of endofunctors, what's the problem?"</em> - from <a href="http://james-iry.blogspot.com/2009/05/brief-incomplete-and-mostly-wrong.html">A Brief, Incomplete, and Mostly Wrong History of Programming Languages</a></blockquote>
</div>



<div class="slide">
<h1>Question: How do you write a big program?</h1>
</div>



<div class="slide">
<h1>Question: How do you write a big program?</h1>
<h1>Answer: By combining multiple little programs.</h1>
</div>



<div class="slide">
<h1>Question: How do you write a big program?</h1>
<h1>Answer: By combining multiple little programs.</h1>
<h1>Question: How do you combine little programs?</h1>
</div>


<div class="slide">
<h1>Question: How do you write a big program?</h1>
<h1>Answer: By combining multiple little programs.</h1>
<h1>Question: How do you combine little programs?</h1>
<h1>Answer: With another program.</h1>
</div>



<div class="slide">
<h1>Question: How do you write a big program?</h1>
<h1>Answer: By combining multiple little programs.</h1>
<h1>Question: How do you combine little programs?</h1>
<h1>Answer: With another program.</h1>
<h1>Question: Wait a minute... what's a program?</h1>
</div>



<div class="slide">
<h1>Question: How do you write a big program?</h1>
<h1>Answer: By combining multiple little programs.</h1>
<h1>Question: How do you combine little programs?</h1>
<h1>Answer: With another program.</h1>
<h1>Question: Wait a minute... what's a program?</h1>
<h1>Answer:</h1>
<p>Back in the 1930's some Mathematicians decided to call "programs"(or algorithms) the set of Computable functions, which are a subset of "all possible functions". Being mathematicians, the next step was to figure out which functions from the set of all possible functions are computable. We know some functions are computable and some are not. In particular, there are two "models" of computable functions: Turing Machines (discovered by Alan Turing), and expressions in the Lambda Calculus (discovered by Alonzo Church). It turns out that these two models are equivalent (every function that can be computed by a Turing Machine has an expression in the Lambda calculus, and every expression in the Lambda calculus can be computed by a Turing machine). Every function that we have found to be computable can be expressed by those models, however, we haven't proved that these models are capable of expressing <em>every</em> computable function. However, it is generally accepted that they do, and this is called the <strong>Church-Turing Thesis</strong>.</p>
</div>



<div class="slide">
<h1>Question: How do you write a big program?</h1>
<h1>Answer: By combining multiple little programs.</h1>
<h1>Question: How do you combine little programs?</h1>
<h1>Answer: With another program.</h1>
<h1>Question: Wait a minute... what's a program?</h1>
<h1>Answer:</h1>
<p>Back in the 1930's some Mathematicians decided to call "programs"(or algorithms) the set of Computable functions, which are a subset of "all possible functions". Being mathematicians, the next step was to figure out which functions from the set of all possible functions are computable. We know some functions are computable and some are not. In particular, there are two "models" of computable functions: Turing Machines (discovered by Alan Turing), and expressions in the Lambda Calculus (discovered by Alonzo Church). It turns out that these two models are equivalent (every function that can be computed by a Turing Machine has an expression in the Lambda calculus, and every expression in the Lambda calculus can be computed by a Turing machine). Every function that we have found to be computable can be expressed by those models, however, we haven't proved that these models are capable of expressing <em>every</em> computable function. However, it is generally accepted that they do, and this is called the <strong>Church-Turing Thesis</strong>.</p>
<h1>Question: Do you have any tylenol?</h1>
</div>



<div class="slide">
<h1>Question: How do you write a big program?</h1>
<h1>Answer: By combining multiple little programs.</h1>
<h1>Question: How do you combine little programs?</h1>
<h1>Answer: With another program.</h1>
<h1>Question: Wait a minute... what's a program?</h1>
<h1>Answer:</h1>
<p>Back in the 1930's some Mathematicians decided to call "programs"(or algorithms) the set of Computable functions, which are a subset of "all possible functions". Being mathematicians, the next step was to figure out which functions from the set of all possible functions are computable. We know some functions are computable and some are not. In particular, there are two "models" of computable functions: Turing Machines (discovered by Alan Turing), and expressions in the Lambda Calculus (discovered by Alonzo Church). It turns out that these two models are equivalent (every function that can be computed by a Turing Machine has an expression in the Lambda calculus, and every expression in the Lambda calculus can be computed by a Turing machine). Every function that we have found to be computable can be expressed by those models, however, we haven't proved that these models are capable of expressing <em>every</em> computable function. However, it is generally accepted that they do, and this is called the <strong>Church-Turing Thesis</strong>.</p>
<h1>Question: Do you have any tylenol?</h1>
<h1>Answer: Ah... nevermind. Programs are functions.</h1>
</div>



<div class="slide">
<h1>So let's go back to combining programs... I mean functions. You said we're going to use functions?</h1>

</div>



<div class="slide">
<h1>A "multithreaded" program.</h1>
<p>Let's start with a simple class to keep a counter.</p>
<pre><code class="java">final class Counter {
	private int counter;
	public Counter(int initial) { this.counter = initial; }
	public int increment() { return this.counter++; }
	public int decrement() { return this.counter--; }
	public int get() { return this.counter; }
	public int set(int value) { return this.counter = value; }
}</code></pre>
<p>Now, we want to make two counters, and "drain" one to another. And we would like to do this concurrently, because maybe we're building a work queue of some sort, and want to keep a list of completed jobs. Now, since there are going to be two counters(or lists) that other threads may be accessing, we want to use a lock to keep other threads from catching us altering the states of both counters. So let's write a simple class to do this. Also, since these things can take some time, we'll use some calls to <code>Thread.sleep(...)</code> to symbolize this.</p>
<pre><code class="java">final class Drain implements Runnable {
	private final Counter source;
	private final Counter sink;
	public Drain(Counter source, Counter sink) {
		this.source = source;
		this.sink = sink;
	}
	@Override public void run() {
		boolean flag = true;
		while(flag) {
			synchronized(source) {
				if(source.get() > 0) {
					synchronized(sink) {
						try { Thread.sleep(150); } catch (InterruptedException ie) { }
						source.decrement();
						sink.increment();
						System.out.println("Drain(" + this + "): " + source.get() + ", " + sink.get());
					}
				} else {
					flag = false;
				}
				try { Thread.sleep(40); } catch (InterruptedException ie) { }
			}
		}
	}
}</code></pre>
</div>

<div>
<h1>Running our multithreaded program "solo"</h1>
<p>So let's make a program to use our classes.</p>
<pre><code class="java">public final class Threads {
	public static void main(String[] args) throws Exception {
		final Counter a = new Counter(5);
		final Counter b = new Counter(10);
		final Drain d1 = new Drain(a, b);
		System.out.println("Counters: " + a.get() + ", " + b.get());
		final Thread t1 = new Thread(d1);
		t1.start();
		t1.join();
		System.out.println("Counters: " + a.get() + ", " + b.get());
	}
}</code></pre>
<p>So the output of this will be something like:</p>
<pre>Counters: 5, 10
Drain(Drain@3be5d207): 4, 11
Drain(Drain@3be5d207): 3, 12
Drain(Drain@3be5d207): 2, 13
Drain(Drain@3be5d207): 1, 14
Drain(Drain@3be5d207): 0, 15</pre>

<p>I think it's fair to say that this small program is actually reasonably correct, and could be used (or "composed") in a larger program and work reliably. But of course, what happens when we write a slightly different program, using two drains?</p>
</div>



<div class="slide">
<h1>Going down the multithreaded drain.</h1>
<p>So now let's write a program to use two drains:</p>
<pre><code class="java">public final class Threads2 {
	public static void main(String[] args) throws Exception {
		final Counter a = new Counter(5);
		final Counter b = new Counter(10);
		final Thread t1 = new Thread(new Drain(a, b));
		final Thread t2 = new Thread(new Drain(b, a));
		t1.start();
		t2.start();
		t1.join();
		t2.join();
		System.out.println("Counters: " + a.get() + ", " + b.get());
	}
}</code></pre>
<p>So what does this program output?</p>
<pre>Counters: 5, 10
Drain(Drain@1f2f0ce9): 4, 11</pre>
<p>Well, at least that's what it's output so far. I wrote this sometime last week, and I am still waiting for it to finish...</p>
</div>



<div class="slide">
<h1>Going down the multithreaded drain.</h1>
<p>So now let's write a program to use two drains:</p>
<pre><code class="java">public final class Threads2 {
	public static void main(String[] args) throws Exception {
		final Counter a = new Counter(5);
		final Counter b = new Counter(10);
		final Thread t1 = new Thread(new Drain(a, b));
		final Thread t2 = new Thread(new Drain(b, a));
		t1.start();
		t2.start();
		t1.join();
		t2.join();
		System.out.println("Counters: " + a.get() + ", " + b.get());
	}
}</code></pre>
<p>So what does this program output?</p>
<pre>Counters: 5, 10
Drain(Drain@1f2f0ce9): 4, 11</pre>
<p>Well, at least that's what it's output so far. I wrote this sometime last week, and I am still waiting for it to finish...</p>
<p>But of course, if you're paying attention, you'll realize that this program will never finish. Feel free to play with the sleep values though, because you can get other incorrect behaviors too (not always reliably incorrect of course...), like thread starvation, and so on. The issue lies in the <code>synchronized</code> blocks. Both drains lock the source first, then the sink. In this case, they each lock the counter the other one needs first, and then start waiting (for forever) until the other lock becomes available, of course, since the other drain holds the lock and is sleeping until the other one releases the lock, this can never happen. Now, I have gone to some effort to make an "obvious" and reliable deadlock (this is a textbook deadlock), but when people say that multithreaded programming is hard to do correctly, this is one example of what is discussed.</p>
</div>



<div class="slide">
<h1>A second act</h1>
<p>Now, this is a group of functional programmers, so clearly the mistake is that I used Java. So let's rewrite this in Erlang. So here's a counter and a drain implementation, along with two functions to start single and double drain versions:</p>
<pre><code class="erlang">-module(threads).
-export([counter/1, drain/2, startOneDrain/0, startTwoDrains/0]).

counter(X, Lock_PID) ->
	receive
		{ counter_set, NewX, Lock_PID } ->
			io:format("counter ~w: ~w~n", [ self(), NewX ]),
			counter(NewX, Lock_PID);
		{ counter_inc, Lock_PID } ->
			io:format("counter ~w: ~w~n", [ self(), X + 1 ]),
			Lock_PID ! { counter_inc, X + 1, self() },
			counter(X + 1, Lock_PID);
		{ counter_dec, Lock_PID } ->
			io:format("counter ~w: ~w~n", [ self(), X - 1 ]),
			Lock_PID ! { counter_dec, X - 1, self() },
			counter(X - 1, Lock_PID);
		{ counter_unlock, Lock_PID } ->
			io:format("counter ~w UNLOCKED ~w~n", [ self(), Lock_PID ]),
			counter(X);
		_ ->
			counter(X, Lock_PID)
	end.

counter(X) ->
	receive
		{ counter_lock, Lock_PID } ->
			io:format("counter ~w locking for ~w: ~w~n", [ self(), Lock_PID, X ]),
			Lock_PID ! { ok, X, self() },
			counter(X, Lock_PID);
		_ ->
			counter(X)
	end.

drain(done) ->
	io:format("Drain finished. ~w~n.", [ self() ]).

drain_sink_ok(Source, Sink) ->
	receive
		{ counter_inc, _, Sink } ->
			Sink ! { counter_unlock, self() },
			Source ! { counter_unlock, self() },
			drain(Source, Sink);
		_ ->
			drain_sink_ok(Source, Sink)
	end.

drain_wait_for_sink(Source, Sink, X) ->
	receive
		{ ok, Y, Sink } ->
			Source ! { counter_dec, self() },
			Sink ! { counter_inc, self() },
			io:format("Draining ~w: ~w to ~w~n", [self(), X, Y]),
			drain_sink_ok(Source, Sink);
		_ ->
			drain_wait_for_sink(Source, Sink, X)
	end.

drain(Source, Sink) ->
	Source ! { counter_lock, self() },
	receive
		{ ok, 0, Source } ->
			Source ! { counter_unlock, self() },
			drain(done);
		{ ok, X, Source } ->
			Sink ! { counter_lock, self() },
			drain_wait_for_sink(Source, Sink, X);
		_ ->
			drain(Source, Sink)
	end.

startOneDrain() ->
	A = spawn(threads, counter, [5]),
	B = spawn(threads, counter, [10]),
	spawn(threads, drain, [ A, B ]).

startTwoDrains() ->
	A = spawn(threads, counter, [5]),
	B = spawn(threads, counter, [10]),
	spawn(threads, drain, [ A, B ]),
	spawn(threads, drain, [ B, A ]).</code></pre>
</div>


<div class="slide">
	<h1>Erlang to the rescue!?</h1>
	<p>So when we start up the Erlang interpreter and compile our module (<code>erl</code> followed by <code>c(threads).</code>) and run the single drain version(do a <code>threads:startOneDrain().</code>), we get something like this:
<pre>1> c(threads).
{ok,threads}
2> threads:startOneDrain().
counter &lt;0.40.0> locking for &lt;0.42.0>: 5
&lt;0.42.0>
counter &lt;0.41.0> locking for &lt;0.42.0>: 10
Draining &lt;0.42.0>: 5 to 10
counter &lt;0.40.0>: 4
counter &lt;0.41.0>: 11
counter &lt;0.41.0> UNLOCKED &lt;0.42.0>
counter &lt;0.40.0> UNLOCKED &lt;0.42.0>
counter &lt;0.40.0> locking for &lt;0.42.0>: 4
counter &lt;0.41.0> locking for &lt;0.42.0>: 11
# omitted ...
Draining &lt;0.42.0>: 1 to 14
counter &lt;0.40.0>: 0
counter &lt;0.41.0>: 15
counter &lt;0.41.0> UNLOCKED &lt;0.42.0>
counter &lt;0.40.0> UNLOCKED &lt;0.42.0>
counter &lt;0.40.0> locking for &lt;0.42.0>: 0
Drain finished. &lt;0.42.0>
counter &lt;0.40.0> UNLOCKED &lt;0.42.0></pre>

<p>Great! So let's try the two drain version! <code>threads:startTwoDrains().</code>, and we get...</p>
<pre>3> threads:startTwoDrains().
counter &lt;0.45.0> locking for &lt;0.47.0>: 5
counter &lt;0.46.0> locking for &lt;0.48.0>: 10
&lt;0.48.0>
4></pre>
<p>Hey... wait a minute... (or 10 years... it's all the same). Erlang's superior actor concurrency, non-shared state, functional style did nothing to prevent me from duplicating the same deadlock bug my Java version had...</p>
</div>



<div class="slide">
<h1>Composing threads</h1>
<p>Multithreaded programs have an interesting property. We can write correct multithreaded programs, and then combine them with other multithreaded programs, that are also correct, and there is no guarantee that larger combined program will be correct. This goes way beyond mutable state. And while some programs assign numerical values to locks and use the ordering imposed by the integer values on locks to have a canonical locking order, there are still other bugs that regularly occur. Many result in security issues.</p>

<p>I have a claim.</p>
<h1>Mulithreaded programming is hard because there is no composition operator that preserves correctness when combining two correct multithreaded programs.</h1>

<p>Actor concurrency, with "shared nothing" memory, is a great and useful thing, but it you can still write programs using actors that can not be combined without bugs(like deadlock, livelock, and friends).</p>
</div>


<div class="slide">
<h1>Question: So wait, that was an example of combining two "smaller" programs into a "bigger" program using functions?</h1>
</div>

<div class="slide">
<h1>Question: So wait, that was an example of combining two "smaller" programs into a "bigger" program using functions?</h1>
<h1>Answer: Yes, that was.</h1>
</div>

<div class="slide">
<h1>Question: So wait, that was an example of combining two "smaller" programs into a "bigger" program using functions?</h1>
<h1>Answer: Yes, that was.</h1>
<h1>Question: It still didn't work though?</h1>
</div>

<div class="slide">
<h1>Question: So wait, that was an example of combining two "smaller" programs into a "bigger" program using functions?</h1>
<h1>Answer: Yes, that was.</h1>
<h1>Question: It still didn't work though?</h1>
<h1>Answer:</h1>
<p>That's because our function that performed the composition lacked a useful property. It didn't "preserve correctness". Ideally, a composition operator(aka function) will preserve some "invariant". However, we can define different invariants that prove different things.</p>

<p>In fact, one of the insights functional programming has helpd bring to the world was the idea that composition should preserve some type of "correctness property. Many research projects have been started to find operators that do preserve correctness. For instance, let's consider another way to write the previous example using <a href="https://bartoszmilewski.com/2010/09/11/beyond-locks-software-transactional-memory/">Software Transactional Memory</a></p>
</div>


<div class="slide">
<h1>Question: So wait, that was an example of combining two "smaller" programs into a "bigger" program using functions?</h1>
<h1>Answer: Yes, that was.</h1>
<h1>Question: It still didn't work though?</h1>
<h1>Answer:</h1>
<p>That's because our function that performed the composition lacked a useful property. It didn't "preserve correctness". Ideally, a composition operator(aka function) will preserve some "invariant". However, we can define different invariants that prove different things.</p>

<p>In fact, one of the insights functional programming has helpd bring to the world was the idea that composition should preserve some type of "correctness property. Many research projects have been started to find operators that do preserve correctness. For instance, let's consider another way to write the previous example using <a href="https://bartoszmilewski.com/2010/09/11/beyond-locks-software-transactional-memory/">Software Transactional Memory</a></p>
<h1>Question: My head is hurting again...</h1>
</div>

<div class="slide">
<h1>Question: So wait, that was an example of combining two "smaller" programs into a "bigger" program using functions?</h1>
<h1>Answer: Yes, that was.</h1>
<h1>Question: It still didn't work though?</h1>
<h1>Answer:</h1>
<p>That's because our function that performed the composition lacked a useful property. It didn't "preserve correctness". Ideally, a composition operator(aka function) will preserve some "invariant". However, we can define different invariants that prove different things.</p>

<p>In fact, one of the insights functional programming has helpd bring to the world was the idea that composition should preserve some type of "correctness property. Many research projects have been started to find operators that do preserve correctness. For instance, let's consider another way to write the previous example using <a href="https://bartoszmilewski.com/2010/09/11/beyond-locks-software-transactional-memory/">Software Transactional Memory</a></p>
<h1>Question: My head is hurting again...</h1>
<h1>Answer: Let's rollback...</h1>
</div>



<div class="slide">
<h1>Threads, this time with commitment.</h1>
TO DO, finish Haskell STM version, possibly SQL version too.
</div>



<div class="slide">
<h1>Question: Okay, so that STM stuff is neat. But I don't write a lot of parallel code.</h1>
</div>

<div class="slide">
<h1>Question: Okay, so that STM stuff is neat. But I don't write a lot of parallel code.</h1>
<h1>Answer: Composition shows up everywhere, let's look at some loops.</h1>
</div>


<div class="slide">
<h1>Question: Okay, so that STM stuff is neat. But I don't write a lot of parallel code.</h1>
<h1>Answer: Composition shows up everywhere, let's look at some simple loops.</h1>
<h1>Question: "Simple"? You keep using that word, I do not think it means what you think it means.</h1>
</div>

<div class="slide">

</div>



<div class="slide">
<h1>Arithmetic (Elementary Algebra)</h1>
<p>Most of us are familiar with numbers, and some basic operations on them like addition, subtraction, multiplication, and division. For better or worse, we even had some terminology inflicted on us in school. So let's review some of those terms and ideas. Please remember, the following slides are discussing properties of addition and multiplication and numbers. Don't try to over think it.</p>

<dl>
	<dt>Infix style: <code>a + 0 = 0 + a = a</code></dt>
	<dt>Prefix style: <code>+(a,0) = +(0,a) = a</code></dt>
	<dd>This is the <em>existence of an identity element</em> property. The idea is that there is an element such that when you combine it with anything else, you just get the same thing. For multiplication, the identity is of course 1 since <code>1 * a = a * 1 = a</code>. For addition, <code>0 + a = a + 0 = a</code>.</dd>

	<dt>Infix style: <code>a + a-inverse = a-inverse + a = 0</code></dt>
	<dt>Prefix style: <code>+(a, inverse(a)) = +(inverse(a), a) = a</code><dt>
	<dd>This is the <em>existence of inverses</em> property. The idea is that for every number or element in the set we have another number or element that acts as it's "inverse", such that when we combine them we get the identity element or number. One important idea that comes out of this is that subtraction and division can be thought of as "non-primitive" operations, and instead the addition (or multiplication) by an "inverse" operator that finds the inverse of an element or number. So <code>a - b = a + inverse(b) = a + b<sup>-1</sup></code>. The "superscripted -1" is often used as notation for "take the inverse" in algebra. Let's look at some examples to help explain: <code>a + -a = 0</code>, with multiplication <code>a * 1/a = 1</code>.</dd>
</dl>
</div>



<div class="slide">
<h1>Arithmetic (Elementary Algebra), part 2</h1>
<dl>
	<dt>Infix style: <code>a + (b + c) = (a + b) + c</code></dt>
	<dt>Prefix style: <code>+(a,+(b,c)) = +(+(a,b), c)</code></dt>
	<dd>This is the <em>associative property</em>. The idea is rather simple: it doesn't matter which pair of numbers we "add" together first, as long as we don't change the order(i.e. grouping doesn't matter). Of course, we might decide to confuse this with commutativity which says "order" doesn't matter, because addition and multiplication are commutative as well. For an example of an operation that is associative but not commutative consider list concatenation: <code>[1] + ([2] + [3]) = ([1] + [2]) + [3] = [1,2,3]</code>, but <code>[1] + [2] != [2] + [1]</code> (because <code>[1,2] != [2,1]</code>)</dd>

	<dt>Infix style: <code>a + b = b + a</code></dt>
	<dt>Prefix style: <code>+(a,b) = +(b,a)</code></dt>
	<dd>This is the <em>commutative property</em>. The idea is that it doesn't matter which parameter to the operation comes first, the result is the same either way. Since we're sticking to "numbers" let's talk about an operation which is commutative but not associative, consider the idea of taking the "midpoint" as a binary operation: <code>mid(a,b) = (a + b) / 2</code>. Now, <code>mid(a,b) = (a + b)/2 = (b + a)/2 = mid(b,a)</code>, so this is commutative, but consider: <code>mid(a,mid(b,c)) = (a + ((b+c)/2)) / 2 = a/2 + b/4 + c/4</code>, and <code>mid(mid(a,b),c) = (((a+b)/2) + c) / 2 = a/4 + b/4 + c/2</code>. So, let a = 2, b = 3, c = 4, and we have 2.75 and 3.25 respectively, so clearly, this is not associative.</dd>
</dl>
</div>



<div class="slide">
<h1>Arithmetic (Elementary Algebra), part 3</h1>
<dl>
	<dt>Infix style: <code>a * (b + c) = a*b + a*c</code></dt>
	<dt>Prefix style: <code>*(a,+(b,c)) = +(*(a,b),*(a,c))</code></dt>
	<dd>This is the <em>distributive property</em>. Previously we were just talking about one operation, the distributive property is about two operations. Of course. This actually isn't entirely true... considering we discussed the idea that some operations are NOT commutative, it's more fair to call this the <strong>left</strong> <em>distributive property</em>. Not every pair of operations is distributive as well, but I'll ignore that for the time being.</dd>

	<dt>Infix style: <code>(a + b) * c = a*c + b*c</code></dt>
	<dt>Prefix style: <code>*(+(a,b),c) = +(*(a,c),*(b,c))</code></dt>
	<dd>This is the <em>right distributive property</em>. For completeness, since not everything is commutative.</dd>

	<dt>Infix style: <code>a = b, b = c implies a = c</code></dt>
	<dt>Prefix style: <code>=(a,b) and =(b,c) implies =(a,c)</code></dt>
	<dd>This is the <em>transitive property</em>. Equality itself is a "binary operation", and one of the most important aspects of it is that it is transitive. But there are other transitive operations: "less than or equal to"(<code>&lt;=</code>) is transitive, so is "greater than or equal to". "is subset of" is also transitive. This idea becomes very important, we'll come back to it later.</dd>

	<dt><code>f(g(x)) = (f &compfn; g)(x)</code></dt>
	<dd>The &compfn; is known as the <em>function composition operator</em>. The previous discussions about these properties have focused on numbers and operations on numbers, but in algebra we talk about "operations" a lot, and function composition is an operation, it even has a symbol.</dd>
</dl>
</div>



<div class="slide">
<h1>Mathematical and Logic symbols</h1>
<p>While the previous discussion of elementary algebra should have been a "review" in some sense, this is probably new material for many people. When reading mathematical texts, especially ones involving logic, there are many symbols that are used in a specific, formal way. However, many of these symbols can be "read" as phrases/words in English that should make sense to many programmers.</p>

<dl>
	<dt><code>&forall;</code></dt>
	<dd>"Universal Quantification". When writing in English you can substitute the phrase <em>for all</em> or <em>for every</em>. It expresses the notion that some statemant is true "for every" value. While it is easy to compare this to "looping" in programming, it is probably more accurate to compare "for all" statements to "loop invariants", i.e. something that will always be "true".</dd>
	<dt><code>&Exists;</code></dt>
	<dd>"Existential Quantification". When writing this in English the phrase <em>there exists</em>, <em>there is</em>, or <em>there are</em> should be used. It can be thought of as implying a search for value that would be successful. There is a value that would satisfy some statement.</dd>
	<dt><code>&Element;</code></dt>
	<dd>When reading this symbol read it as "element of", "in".</dd>
	<dt><code>&top;</code></dt>
	<dd>This symbol is called "Top", think of it as a "penultimate maximum" value.</dd>
	<dt><code>&bot;</code></dt>
	<dd>This symbol is called "Bottom", think of it as a "penultimate minimum" value.</dd>
	<dt><code>&VerticalBar;</code> also <code>:</code></dt>
	<dd>This symbol is read as <em>such that</em> or <em>where</em>. It is used to place a condition or predicate (the right side) on the expression on the left side.</dd>
	<dt><code>&and;</code></dt>
	<dd>"Conjunction". <em>and</em>. Really, just a logical "and".</dd>
	<dt><code>&or;</code></dt>
	<dd>"Disjunction". <em>or</em>. Really, just a logical "or".</dd>
	<dt><code>&Implies;</code></dt>
	<dd>"Implication". When read this symbol use the word "implies" or "if - then". Basically, if the left hand side is true, then the right hand side is entailed or implied to be true as well.</dd>
	<dt>(a horizontal bar)</dt>
	<dd>"entails", "if/then", or "judgement". In logic, this is a way of expressing a conditional truth. Two expressions are written, one on top of the bar, the other on the bottom. If the top expression is true, then the bottom one is true as well, i.e. the bottom statement is entailed by the (truth of) the top statement.</dd>
</dl>
</div>



<div class="slide">
<h1>Sets</h1>
<p>In modern mathematics, sets have been formalized, but the idea is intuitive: a set of <em>things</em> has a notion of membership, a thing either is or is not a member of a set. There is no notion of order though. There is no "first" member of a set. However, we can frequently define sets (or subsets of sets) using some "predicate" to determine membership. For instance the "the set of all numbers evenly divisible by 2" is a set. Sets are often written as a list of things in between { and }, so <code>{ 1, 2, 3, 4 }</code> would be a set. The set with nothing in it is called the <em>empty set</em>, and it is often written: <code>&emptyset;</code>, however it can also be written <code>{}</code>. "Predicates" are mathematical statements whose values are either true or false. Sets that are defined in terms of another set and "predicates" are often written in a mathematical notation like so:
<code>Evens = { x &Element; &integers; &VerticalBar; x mod 2 = 0 }</code>.
Which if read in "English" says: <em>"The set 'Evens' is equal to the set x's in the Integers such that x modulo 2 is equal to 0."</em></p>

<h2>Operations on Sets</h2>
<dl>
	<dt><code>A &Union; B = { x &VerticalBar; x &Element; A or x &Element; B }</code></dt>
	<dd>This is the <em>set union</em> operator. It produces a new set. An element is a member of the new set if it was a member of at least one of the sets we combined.</dd>
	<dt><code>A &Intersection; B = { x &VerticalBar; x &Element; A and x &Element; B }</code></dt>
	<dd>This is the <em>set intersection</em> operator. It produces a new containing only elements that were in both sets.</dd>
	<dt><code>A &subset; B = true if and only if &forall; x &Element; A, x &Element; B</code></dt>
	<dd>The <em>subset</em> relationship is a binary operator like equality that is true if all the elements of A are also elements of B.</dd>
	<dt><code>A = B if A &subset; B and B &subset; A</code></dt>
	<dd>Two sets are considered equal if they are subsets of each other (i.e. every element of A is also in B, and every element in B is also in A).</dd>
	<dt><code>A &setminus; B = { x &Element; A &VerticalBar; x &notin; B }</code></dt>
	<dd><em>Set difference</em> or <em>complement</em> is an operator that gives a subset A containing only the elements that were not also in B. This could actually be the empty set (<code>&emptyset;</code>).</dd>
	<dt><code>A &Cross; B = { (a,b) &VerticalBar; a &Element; A, b &Element; B }</code></dt>
</dl>
</div>



<div class="slide">
<h1>Tuples</h1>
<p>A n-tuple is just an ordered "list" of <code>n</code> things. You can also have a set of tuples(in fact, tables in SQL are sets of n-tuples). A 2-tuple is often just called an "ordered pair". However, in ordinary mathematics, we like to reduce everything to sets, so I want to suggest we can encode "n-tuples" as sets. Now, the ordered pair: <code>(a,b)</code> can't simply be written as the set <code>{ a, b }</code>, because in a set order doesn't matter. But we can tell if two sets are equal to each other, we can also take the difference of two sets. So we can define a tuple(or "encode a tuple as a set") as follows:
<code>P = (a, b) = { {a}, {a,b} }</code>
And we can even define expressions in terms of set operations to give us the first and second elements.</p>

<dl>
	<dt><code>first(p) = &Union; &Intersection; p</code></dt>
	<dd>Breaking this down: the set of all x's such that for every element of the Y of the set P, x is an element of Y. Since the "first" element of the tuple will be in both sets, that is the only value that can satisfy this predicate.</dd>
	<dt><code>second(p) = &Union; { x &Element; &Union; p &VerticalBar; &Union; p &NotEqual; &Intersection; p &RightArrow; x &NotElement; &Intersection; p }</code></dt>
	<dd></dd>
</dl>

<p>This "technique" can be extended for any arbitrary "n"-tuples. In true mathematician style I will leave this as an exercise to the reader.</p>
</div>



<div class="slide">
<h1>Programming and sets</h1>
<p>Recall the subset notation previously. In Python we could write that as this code:
<pre><code class="python">Z = range(4294967295) # we'll pretend the maximum value for a 32-bit integer is "all integers"
Evens = [ x for x in Z if x % 2 == 0 ]</code></pre>
Those familiar with Python will recall this is called a "list comprehension". The general idea is to apply some bit of code to each element of a list to determine if the element is in the list.</p>

<p>In (PostgreSQL) SQL we could write:
<pre><code class="sql">SELECT x
    FROM (SELECT generate_series(0, 4294967295) AS x) Z
    WHERE x % 2 = 0;</code></pre>
Of course, tables and queries in SQL are sets of n-tuples(more on that soon), so this is probably not too surprising.</p>

<p>In Haskell we could write:
<pre><code class="haskell">Evens = [x | x <- [1..], mod x 2 == 0 ]</code></pre></p>

<p>These examples were picked intentionally because mathematical notation, language, and concepts permeate many programming languages. But it goes even deeper than that.</p>
</div>



<div class="slide">
<h1>Functions and Relations</h1>
<p>Now that we know about sets and n-tuples, we can talk about <em>relations</em>. In mathematics, a relation is quite simply a set of n-tuples, where n is the same for every element of the set.</p>
<table>
<tr><td><code>(1, 2, 4)</code></td></tr>
<tr><td><code>(2, 4, 4)</code></td></tr>
<tr><td><code>(6, 4, 5)</code></td></tr>
<tr><td><code>(4, 2, 1)</code></td></tr>
<tr><td><code>(7, 9, 10)</code></td></tr>
</table>

<p>A <em>function</em> is a relation in which one element of the n-tuple is denoted the "key" and the rest of the tuple is denoted as the "value". The set of values that comprise the key's are called the <em>domain</em>, the set of the rest of the values from the n-tuple is called the <em>codomain</em>. Or to use another idea, the <em>domain</em> comes from one set, and the <em>codomain</em> comes from another set, and the relation/function is comprised of a subset of the cross product of the two sets.</p>

<h2>Combining Relations</h2>
<p>Now that we have some definitions, let's talk about combining relations (or functions). Given two relations that consist of ordered pairs, <code>F</code>(on the sets <code>Y &Cross; Z</code>) and <code>G</code>(on the sets <code>X &Cross; Y</code>), let's define a <code>combine</code>. Create a new set of ordered pairs, matching up the second element(using the <code>second</code> operation we defined on ordered pairs) of the pairs in G with the first element of the ordered pairs in F. Or:
<pre><code>combine(F,G) = { (x,z) &Element; X &Cross; Z &VerticalBar; &Exists; y &Element; Y : (x,y) &Element; G &and; (y,z) &Element; F }</code></pre></p>

<p>We can extend this to arbitrary n-tuples, but the idea is the same. In fact, we can realize that <code>combine</code> is simply the function composition operator we discussed earlier.</p>
</div>



<div class="slide">
<h1>Relational Algebra</h1>
<p>Many programmers have probably heard that SQL is based on something called the <em>relational algebra</em>. It's probably worth pointing out now that the mathematical definition of a <em>relation</em> is identical to the SQL definition. This is not an accident. In fact, SQL also has the function composition operator: <code>JOIN</code>:
<pre><code class="sql">SELECT x,z
&nbsp;&nbsp;&nbsp;&nbsp;FROM F
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;JOIN G
&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;&nbsp;ON (F.y = G.y);</code></pre>
(minor footnote: this only if the join condition is a simple equality... join is actually more flexible than function composition, of course, it is in fact a composition operator for creating new relations out of other relations... and that's what counts.)</p>

<p>Cross products in SQL are simply the <code>CROSS JOIN</code> operator. And of course, since relations are just sets of n-tuples, we have all the set operators available too, set unions(<code>UNION</code>):
<pre><code class="sql">(SELECT * FROM F) UNION (SELECT * FROM G)</code></pre></p>
<p>Set intersection(<code>INTERSECT</code>):
<pre><code class="sql">(SELECT * FROM F) INTERSECT (SELECT * FROM G)</code></pre></p>
<p>and set difference(<code>EXCEPT</code>):
<pre><code class="sql">(SELECT * FROM F) EXCEPT (SELECT * FROM G)</code></pre></p>
<p>We even have the <code>&Element;</code> notation in the form of <code>IN</code>:
<pre><code class="sql">SELECT * FROM F WHERE (x,y) IN (SELECT * FROM G);</code></pre></p>
</div>



<div class="slide">
<h1>Pure Functions</h1>
<p>Up to this point, I have been discussing <em>mathematical functions and relations</em>. However, programmers often deal with functions that involve some notion of "state", and thus, often have "functions" that have some internal state. A pure function is one in that, when provided with the same parameters always maps to the same value.</p>

<p>One reason to prefer stateless or pure functions is that state transitions cause a combinatorial "explosion" in terms of possibilities. In a system with 2 states, we have to consider the transitions from state 1 to 2, and 2 to 1. However, adding a single state to this, increases the possibilities to 6 (1 to 2, 1 to 3, 2 to 1, 2 to 3, 3 to 1, 3 to 2), and so on.</p>

<p>However, we can turn all functions with "state" into pure functions by simply adding a parameter whose value is the "state" of the system. And the value "returned" by the function will be the new state of the system. This transformation is called <em>world passing style</em>.</p>

<p>One interesting observation is that this turns a function of a single parameter into a binary operation. Since these functions are "pure": <code>op(a, b)</code> will always result in the same value. So now a sequence of the same operation can be written in the following way: <code>op(op(a,b), c)</code> where <code>b</code> and <code>c</code> are inputs, and <code>a</code> is initial state.</p>

<p>Considering that we are discussing algebraic operations, it would be interesting to contemplate whether or not these "world passing style" functions should be associative. In other words, should it be the case that: <code>op(op(a, b), c) = op(a, op(b, c))</code>. (This is an exercise left for the reader, although we'll come back to this later.)</p>
</div>

<div class="slide">
<h1>Transitivity</h1>

</div>


<div class="slide">
<h1>Abstract Algebra</h1>
<p>Elementary algebra looks at symbol manipulation involving operations over numbers. Abstract algebra is the branch of the mathematics that looks at generalized versions of these ideas over different structures, and identifies recurring patterns. From a programming perspective, this is a bit like "duck typing". The general forms(groups, rings, fields, lattices) are identified by the basic properties they have. Once an object is identified with these rules, one can reason about the abstract forms with little regard for the underlying specific instance.</p>
</div>

<div class="slide">
</div>

<div>
Fixed points
Morphisms
Maps
Bijective
Injective
Equivalence Classes
Lattices
Domains
Euclid's Algorithm

http://www.stephendiehl.com/posts/abstraction.html?HN_20160519

https://en.wikipedia.org/wiki/Morphism
https://en.wikipedia.org/wiki/Group_homomorphism
https://en.wikipedia.org/wiki/Map_(mathematics)
https://simple.wikipedia.org/wiki/Relation_(mathematics)
http://www.regentsprep.org/regents/math/algtrig/atp5/lfunction.htm
https://en.wikipedia.org/wiki/Ordered_pair

https://en.wikipedia.org/wiki/Monoid
https://en.wikipedia.org/wiki/Semigroup
https://en.wikipedia.org/wiki/Commutative_property#Noncommutative_operations_in_mathematics
https://en.wikipedia.org/wiki/Group_(mathematics)
https://en.wikipedia.org/wiki/Ring_(mathematics)
http://mathworld.wolfram.com/EquivalenceClass.html


https://en.wikipedia.org/wiki/Intuitionistic_type_theory
https://en.wikipedia.org/wiki/Homotopy_type_theory
https://homotopytypetheory.org/blog/
https://en.wikipedia.org/wiki/Universal_algebra
https://en.wikipedia.org/wiki/Abstract_algebra
https://en.wikipedia.org/wiki/List_of_abstract_algebra_topics
</div>



<div class="slide">
<h1>References</h1>
<ul>
<li><a href="http://worrydream.com/refs/Backus-CanProgrammingBeLiberated.pdf">Can Programming Be Liberated from the von Neumann Style? A Functional Style and Its Algebra of Programs.</a> John Backus, 1977, Turing Award.</li>
<li><a href="https://www.cs.umd.edu/class/spring2003/cmsc838p/Design/criteria.pdf">On the criteria to be used in decomposing systems into modules</a> - D. L. Parnas, 1979.</li>
<li><a href="https://bartoszmilewski.com/2010/09/11/beyond-locks-software-transactional-memory/">Beyond Locks: Software Transactional Memory</a></li>
<li><a href="http://research.microsoft.com/en-us/um/people/simonpj/papers/stm/stm.pdf">Composable Memory Transactions</a></li>
<li><a href="http://www.usingcsp.com/cspbook.pdf">Communicating Sequential Processes</a>, C.A.R. Hoare</a></li>
</ul>
</div>

</body>
</html>
